---
title: "Intro to research toolbox"
author: Jeremy Lew
date: last-modified
date-format: long
format:
  revealjs:
    slide-number: true
    embed-resources: true
    controls: true
    controls-layout: edges
    template-partials:
      - title-slide.html
execute:
  echo: true
bibliography: references.bib
csl: vancouver-superscript.csl 
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, magrittr, here, palmerpenguins, kableExtra, ezyr)
```

## Topics

1.  Introduction & learning plan
2.  Getting comfortable with R
3.  Programming fundamentals
4.  Data cleaning
5.  Data visualisation
6.  Statistical analysis
7.  Presentation of results


# Introduction & learning plan

## Motivation

1.  Added tools to your toolbox
2.  Transferrable skill
3.  Learning how to program in 1 language helps you learn other languages
4.  Leverage on open source tools/projects written by others

## Learning plan {.smaller}

1.  The topics in this deck mirror the workflow of a project

```{mermaid}
%%| echo: false
%%| fig-width: 10

flowchart LR
  D[Data exported\nfrom REDCAP]-->C[Data cleaning]
  C-->S[Statistical analysis]
  S-->P[Presentation\nof results]
```

2.  The game plan is to first do a quick pass through from data cleaning to analysis:
    i.  Take a toy dataset
    ii. "Clean" the data (minimally)
    iii. Run a regression model
3.  Once we have gotten comfortable with R, we will explore each topic in greater detail
    i.  [Data cleaning](#data_cleaning)
    ii. [Programming fundamentals](#prog_funda)
    iii. [Analysis: generalised linear models](#ana_glm)
    iv. [Presentation of results](#presentation)

## Resources

1.  [R courses](https://pll.harvard.edu/subject/r){target="_blank"}
2.  [The Epidemiologist R Handbook](https://epirhandbook.com/en/index.html){target="_blank"}
3.  [R for Data Science](https://r4ds.had.co.nz/index.html){target="_blank"}
4.  [Advanced R](https://adv-r.hadley.nz/){target="_blank"}
5.  [Cheat Sheets](https://posit.co/resources/cheatsheets/){target="_blank"}

# Getting comfortable with R

## What is an R package? {.smaller}

1.  It is a collection of code/data written by someone and packaged for distribution
2.  Hosted on the [*Comprehensive R Archive Network*](https://CRAN.R-project.org/package=palmerpenguins){target="_blank"} and/or [*Github*](https://github.com/allisonhorst/palmerpenguins/){target="_blank"} for public download
3.  From the CRAN webpage, we are able to find a reference manual documenting the data/functions of the package
4.  From the Github repository, we are able to see the actual codes implemented by the package's author(s)
5.  To install a package we would run `install.packages("palmerpenguins")` in the console
6.  To load a package, we would run `library(palmerpenguins)`
7.  To uninstall a package we would run `remove.packages("palmerpenguins")` in the console


## \[DEMO\] Intro to RStudio workspace {.smaller}

1.  Console: where you run/execute lines of code
1.  To assign a variable, we use the `<-` operator
    ```{r}
    x <- 1:10; print(x)
    ```
    i. random-access memory is allocated when a variable is assigned/declared
    i. random-access memory is freed-up when the variable is deleted (or garbage collected if it goes out of scope)
1.  Environment: where you're able to see variables stored in random-access memory
    i.  To clear variables from environment $\rightarrow$ execute `rm(list = ls())` in console

## \[DEMO\] Intro to RStudio workspace {.smaller}

1.  Script
    i. Writing code and saving the script
    i. Execute single/multiple lines of code $\rightarrow$ highlight code + Ctrl-enter
    i. Runs from top to bottom
    i. Comments will not be executed
1.  Hotkeys
    i. Clear the console $\rightarrow$ Ctrl-l
    i. Clear a line in console $\rightarrow$ Ctrl-d


## \[PRACTISE\] Tasks to do {.smaller}

1.  For this task, you will need the `palmerpenguins` and `tidyverse` packages. Install them if you have not already done so
2.  We will make use of the `penguins` dataset from `palmerpenguins` package
3.  Understand the data
4.  Dichotomise `body_mass_g` (continuous) into a categorical variable with 2 categories ("light": \<= 4750g and "heavy": \> 4750g)
5.  Run a linear regression[^1] with `body_mass_g` as the dependent variable and independent variables: `species`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm` and `sex`
6.  Run a logistic regression with the categorical body mass variable you created earlier as the dependent variable, with the same independent variables as before

[^1]: Note: the model is for learning purpose. No clever reason for choosing the variables




# Programming Fundamentals {#prog_funda}

## Data types {.smaller}

::: {.incremental}
1.  Character data type e.g. `"a"`
    ```{r}
    #| output-location: column-fragment
    typeof("a")
    ```
2.  Integer data type e.g. `1L`
    ```{r}
    #| output-location: column-fragment
    typeof(1L)
    ```
3.  Numeric data type e.g. `5.0`

    ```{r}
    #| output-location: column
    typeof(5.0)
    ```
4.  Logical data type e.g. `TRUE` or `FALSE`
    ```{r}
    #| output-location: column
    typeof(TRUE)
    ```
5.  Complex data type e.g. `1 + 4i`
    ```{r}
    #| output-location: column
    typeof(1 + 4i)
    ```
:::

## Data types (continued) {.smaller}
1.  Factor datatype for working with categorical variables
    ```{r}
    x <- factor(sample(c("Malay", "Others", "Indian", "Chinese"), 10, replace = T),
                levels = c("Chinese", "Malay", "Indian", "Others"))
    print(x)
    ```
    ```{r}
    print(levels(x))
    ```

1. A factor variable is more useful than a plain character vector. 
E.g. the first level will be used by `glm` as the reference category




## Data structures (vectors) {.smaller}

::: {.incremental}
1. Data structures are *containers* for holding data
1.  A character vector holds multiple characters
    ```{r}
    #| output-location: column
    letters
    ```
    ```{r}
    #| output-location: column
    typeof(letters)
    ```
1.  A numeric vector holds multiple numbers
    ```{r}
    #| output-location: column
    x <- 1:5; print(x)
    ```
    ```{r}
    #| output-location: column
    typeof(x)
    ```
:::

. . . 

::: callout-note
## Exercise
::: {.incremental}
1. Can you query the length of your character vector? `length(letters)`
1. Can you slice the character vector to get the first 5 elements? `letters[1:5]`
1. Can you slice the character vector to get the 1st, 9th, 20th elements? `letters[c(1, 9, 20)]`
1. Can you flip the character vector from last to first element? `letters[length(letters):1]`
:::
:::


## Data structures (lists) {.smaller}
1. Vectors can only hold 1 kind of data type e.g. characters, numeric etc
1. Lists are “containers” that can hold multiple data types
    1. Unnamed lists
       ```{r}
       list("a", 1, 10:15)
       ```
    1. Named lists
       ```{r}
       list(a = "a", b = 1, c = 10:15)
       ```

## Data structures (lists, continued)

::: callout-note
## Exercise

1. Create a variable in your console: `mylist <- list(a = "a", b = 1, c = 10:15)`
1. Can you query the length of mylist? `length(mylist)`
1. Can you get the second element of your mylist using
    1. index: `mylist[1]`
    1. key: `mylist$a` or `mylist[["a"]]`
1. Compare the difference between `mylist[3]` and `mylist[[3]]`
1. Compare the difference between `class(mylist[3])` and `class(mylist[[3]])`
:::

## Data structures (dataframe) {.smaller}

::: incremental
1. Dataframe is a *tabular container* that can hold multiple data types like lists, but each column can only store data of the same data type
1.  To view the first 2 rows of the `penguins` dataset from the `palmerpenguins` package

    ```{r}
    #| output-location: fragment
    #| code-line-numbers: "|3"
    library(tidyverse)
    library(palmerpenguins)
    penguins %>% head(2)
    ```

2.  To view the last 2 rows of the dataset

    ```{r}
    #| output-location: fragment
    penguins %>% tail(2)
    ```

3.  To view the data in rstudio, execute `view(penguins)`
:::

## Data structures (dataframe, continued)

::: callout-note

## Exercise
1. Can you query the dimensions of the `penguins` dataset using `dim(penguins)`, `ncol(penguins)`, `nrow(penguins)`?
1. Can you get a glimpse of the data using the functions `glimpse(penguins)`?
1. Can you view summary statistics of the data using `summary(penguins)`?
:::

## Data structures (dataframe, accessing variables) {.smaller}

1.  To get the column/variable-names of your dataset
    ```{r}
    colnames(penguins)
    ```
1.  To access any column variable, we use the `$` syntax
    ```{r}
    penguins$species[1:10]
    ```
1.  To get a table count of a variable or a cross-table count of 2 variables
    ```{r}
    table(penguins$sex, useNA = "ifany")
    ```
1. See [Data Cleaning](#data_cleaning) for more ways to work with dataframes


## Data structures (dataframe, loading data) {.smaller}

1. To import data from csv file, use `read_csv`
1. To import data from an excel file, use `read_excel`  
1. To import data from SAS/SPSS/Stata or export from R to these file formats,
use the [`haven`](https://haven.tidyverse.org/){target="_blank"} package


## Data structures (dataframe, adding labels) {.smaller}

1. To add labels to a dataframe, we make use of the [`labelled`](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html){target="_blank"} package



## Functions {.smaller}

1. A good chapter on explaining functions is [Chapter 6, Advanced R](https://adv-r.hadley.nz/functions.html){target="_blank"}, where most of the examples in the slides are taken
1.  Example function
    ```{r}
    f01 <- function(x, y) {
      # A comment
      x + y
    }
    ```
1. Components of a function
    a. *Arguments* $\rightarrow$ inputs to the function (there can also be functions with no arguments)
    a. *Body* $\rightarrow$ the code inside the function
    a. *Environment* $\rightarrow$ a list-like structure that stores/maps the variable name to value (a namespace as some people call it)
1. Functions are objects that can be assigned or they can be anonymous functions

## Function invocation {.smaller}
1.  How do we invoke the function?
    ```{r}
    print(f01)
    f01(3, 5)
    ```

1. How do we invoke one function after another?
    a. Suppose we have another function `f02`
       ```{r}
       f02 <- function(a, b) {
         a * 10 + b
       }
       ```
    b. To call `f01` first followed by `f02` and `sqrt` we will "nest" the functions (inside-out, right to left)
       ```{r}
       sqrt(f02(f01(1, 2), 5))
       ```

## Function invocation (continued) {.smaller}    

Another way to invoke a series of functions is to use the *pipe* operator `%>%` provided by the `magrittr` package
```{r}
library(magrittr)
value <- 1
value %>%
  f01(2) %>%
  f02(5) %>%
  sqrt()
```
    
::: callout-note
## Exercise
1. What is the difference between `f02` and `f02(1, 2)`?
:::

## Function output
1. What is returned by a function? The last evaluated expression

1. How do we store values outputted from a function? With assignment statment

1. Early termination of a function with a return statement

## Functions - argument passing {.smaller}

1. Pass by copy vs pass by reference
1. In R, all functions are pass by reference if you don't modify the object subsequently within the function.
i.e. copy-on-modify
1.  How to pass arguments?
    ```{r}
    f04 <- function(x, y, z) {
      (x / y) * z
    }
    ```
    a. By position e.g. `f04(10, 50, 20)`
    b. By name e.g. `f04(z = 20, y = 50, x = 10)`
    c. Unpacking multiple named arguments using `do.call`
       ```{r}
       do.call(f04, list(x = 10, y = 50, z = 20))
       ```

## Functions - lexical scoping, name masking 

Names defined inside a function mask names defined outside a function

```{r}
x <- 10
y <- 20
f05 <- function() {
  x <- 1
  y <- 2
  c(x, y)
}

f05()
```

## Functions - lexical scoping, looking one level up
R will look up a variable in the enclosing scope (e.g. within the function) and if it's not found,
will continue to proceed upwards (e.g. enclosing function or global environment) to look for
the variable until it is found 
```{r}
x <- 1
f06 <- function() {
  y <- 2
  i <- function() {
    z <- 3
    c(x, y, z)
  }
  i()
}

f06()
```

::: callout-note
## Exercise
Using the sample function, change the z variable inside the function and observe that...
```{r}
z <- 10
f07 <- function(x, y) {
  # make your edit here
  z <- z * 100
  z + x + y
}
```
...there is no change to the z variable outside the function in the global environment
:::

## Functions - lexical scoping, dynamic lookup 

R looks for the values when the function is run, not when the function is created
```{r}
z <- 10
f08 <- function(x, y) {
  z + x + y
}

z <- 100
f08(5, 10)
```

## Functions

## Loops

## Debugging

## Memory Management


# Data Cleaning {#data_cleaning}

## Common functions - packages

1.  These functions are from various packages, conveniently collected in the [tidyverse](https://www.tidyverse.org/){target="_blank"} package
2.  The best package for data wrangling is [dplyr](https://dplyr.tidyverse.org/){target="_blank"}. Useful info can be found in this [chapter](https://r4ds.had.co.nz/transform.html){target="_blank"}
3.  A package for working with dates is [lubridate](https://lubridate.tidyverse.org/){target="_blank"}
4.  A package for working with strings is [stringr](https://stringr.tidyverse.org/){target="_blank"}. This is useful for cleaning free-text response data
5.  A package for working with factor datatype is [forcats](https://forcats.tidyverse.org/){target="_blank"}


## Common functions - dplyr::select {.smaller}

The [`select`](https://dplyr.tidyverse.org/reference/select.html){target="_blank"} function enables you to select a subset of the columns in your dataset

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|4"
# library(tidyverse) # if not already loaded
# library(penguins) # if not already loaded
penguins %>%
  select(species, island, bill_length_mm) %>%
  head(3)
```

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|3"
# there is also ends_with
penguins %>%
  select(starts_with("bill")) %>%
  head(3)
```

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|2"
penguins %>%
  select(matches("mm")) %>%
  head(3)
```

## Common functions - dplyr::filter {.smaller}

The [`filter`](https://dplyr.tidyverse.org/reference/filter.html){target="_blank"} function enables you to select a subset of the rows that meet a certain criteria

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  filter(body_mass_g > 3500) %>%
  head(3)
```

. . .

```{r}
#| output-location: fragment
penguins %>% filter(sex == "female")
```

## Common functions - dplyr::mutate {.smaller}

The [`mutate`](https://dplyr.tidyverse.org/reference/mutate.html){target="_blank"} function enables you to add columns to your dataset. The added columns can be derived from existing column(s)

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  mutate(body_mass_100g = body_mass_g / 100) %>%
  select(body_mass_g, body_mass_100g) %>%
  head(5)
```

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  mutate(bill_length_plus_depth_mm = bill_length_mm + bill_depth_mm) %>%
  select(matches("bill")) %>%
  head(5)
```

## Common functions - dplyr::group_by, summarise {.smaller}

::: incremental
1.  The [`summarise`](https://dplyr.tidyverse.org/reference/summarise.html){target="_blank"} function enables you to get summary statistics like N, mean, median etc

    ```{r}
    #| output-location: fragment
    penguins %>% summarise(N = n(),
                           mean_body_mass_g = mean(body_mass_g))
    ```

2.  The [`group_by`](https://dplyr.tidyverse.org/reference/group_by.html){target="_blank"} function enables you to get summary statistics within groups

    ```{r}
    #| output-location: fragment
    #| code-line-numbers: "|2"
    penguins %>%
      group_by(sex) %>%
      summarise(N = n(),
                mean_body_mass_g = mean(body_mass_g),
                median_body_mass_g = median(body_mass_g))
    ```
:::

## Common functions - dplyr::arrange

The function [`arrange`](https://dplyr.tidyverse.org/reference/arrange.html){target="_blank"} enables us to sort by a certain variable

```{r}
#| output-location: fragment
#| code-line-numbers: "|4"
penguins %>%
  group_by(sex, year) %>%
  summarise(mean_body_mass_g = mean(body_mass_g)) %>%
  arrange(desc(year))
```

## Common functions - dplyr::if_else, case_when {.smaller}

The functions [`if_else`](https://dplyr.tidyverse.org/reference/if_else.html#arguments){target="_blank"} and [`case_when`](https://dplyr.tidyverse.org/reference/case_when.html){target="_blank"} are often used with mutate to create new variables

. . .

```{r}
#| output-location: fragment
#| code-line-number: "|3"
set.seed(2)
penguins %>%
  mutate(bill_length_type = if_else(bill_length_mm >= 48.5, "long bill", "short bill")) %>%
  sample_n(4)
```

. . .

```{r}
#| output-location: fragment
penguins %>%
  mutate(bill_length_type = case_when(bill_length_mm >= 48.5 ~ "long bill",
                                      bill_length_mm < 48.5 ~ "short bill",
                                      TRUE ~ NA_character_)) %>%
  select(bill_length_type) %>%
  table(useNA = "ifany")
```

## Common functions - forcats::fct_relevel {.smaller .scrollable}

1. If we convert a character vector to a factor vector, the levels will be arranged in alphabetical order
   automatically if unspecified
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>% factor()
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. We can specify the levels of the factor using the `levels` argument
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         factor(levels = c("short bill", "medium bill", "long bill"))
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. The factor vector will only contain values specified by `levels`.
   Any existing character values unspecified in `levels` (e.g. "long bill") will be converted to NA
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         factor(levels = c("short bill", "medium bill"))
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. [fct_relevel](https://forcats.tidyverse.org/reference/fct_relevel.html){target="_blank"} relevels levels of a factor vector (if not already a factor eg. character vector, it will first convert it to a factor vector) Any unspecified levels will follow after specified ones, left in the existing order
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         fct_relevel("short bill", "medium bill")
     ) %>%
     .$bill_length_type %>% levels()
   ```

## Common functions - forcats::fct_collapse {.smaller .scrollable}

1. We can collapse levels of a factor variable using forcat's [fct_collapse](https://forcats.tidyverse.org/reference/fct_collapse.html){target="_blank"} function
1. The `island` variable has 3 levels
   ```{r}
   penguins$island %>% table()
   ```
1. Suppose we want to collapse `island` into 2 levels, we can apply `fct_collapse` like so
   ```{r}
   penguins %>%
     mutate(island = fct_collapse(island, "Dream/Torgersen" = c("Dream", "Torgersen"))) %>%
     .$island %>%
     table()
   ```

## Common functions - tidyr::pivot_wider {.smaller .scrollable}

1. Suppose we have some data in long format

   ```{r}
   # Generate fake patient data for illustration
   generate_fake_patient_data <- function(id) {
     data.frame(list(
       patient_id = id,
       time = c("baseline", "12mth"),
       hba1c = runif(n = 2, min = 4, max = 14),
       ldl = sample(c("good control", "bad control"))
     ))
   }
   
   set.seed(2024)
   data <- map(
     sprintf("P%s", str_pad(1:344, width = 2, pad = 0)),
     generate_fake_patient_data
   ) %>%
     bind_rows()
   print(head(data, 10))
   ```

1. We can make use of tidyr's [pivot_wider](https://tidyr.tidyverse.org/reference/pivot_wider.html){target="_blank"} to reshape our data
   from long to wide
   ```{r}
   data_wide <- data %>%
     pivot_wider(id_cols = patient_id,
                 names_from = "time",
                 names_glue = "{.value}_{time}",
                 values_from = c(hba1c, ldl)) 
   head(data_wide, 15)
   ```

## Common functions - tidyr::pivot_longer {.smaller .scrollable}

1. Suppose instead that we were given data in a wide format
   ```{r}
   print(head(data_wide, 10))
   ```

1. We can make use of tidyr's [pivot_longer](https://tidyr.tidyverse.org/reference/pivot_longer.html){target="_blank"}
   to reshape our data into long format
   ```{r}
   data_wide %>%
     pivot_longer(
       cols = matches("baseline|12mth"),
       names_pattern = "(.*)_(baseline|12mth)",
       names_to = c(".value", "time")
     )
   ```


## Common functions - dplyr::bind_rows, bind_cols 

## Common functions - dplyr::mutate + across {.smaller .scrollable}

1. This combination of `mutate` & `across` enables us to apply a function to multiple columns of a dataframe
   ```{r}
   penguins %>%
     mutate(across(.cols = c(bill_length_mm, bill_depth_mm), 
                   .fns = ~ floor(.x),
                   .names = "{.col}_floored")) %>%
     select(matches("bill_(length|depth)"))
   ```

1. If we omit the `.names` argument, the function will be applied in-place
   ```{r}
   penguins %>%
     mutate(across(.cols = c(bill_length_mm, bill_depth_mm), 
                   .fns = ~ floor(.x))) %>%
     select(matches("bill_(length|depth)"))
   ```

## Common functions - stringr::str_detect {.smaller .scrollable}

1. We can make use of [regular expressions](https://rstudio.github.io/cheatsheets/strings.pdf){target="_blank"} in
   stringr's [str_detect](https://stringr.tidyverse.org/reference/str_detect.html){target="_blank"} function together with 
   `filter`, or `if_else`/`case_when` for data cleaning

1. Here we create a `species_plus` variable
   ```{r}
   set.seed(2024)
   penguins <- penguins %>%
     mutate(across(
       .cols = "species",
       .fns = ~ paste(.x,
         sample(c("baby", "teenager", "adult"), nrow(penguins), replace = TRUE),
         sex,
         sep = "_"
       ),
       .names = "{.col}_plus"
     ))
   # view counts of species_plus
   table(penguins$species_plus)
   ```

1. We can use a regular expression to identify categories
   ```{r}
   penguins %>%
     mutate(across(
       .cols = "species_plus",
       .fns = ~ case_when(
         # Chinstrap/Gentoo, not followed by baby, male/female
         str_detect(.x, "((Chinstrap|Gentoo)(?!_baby)).*(male|female)") ~ "DO SOMETHING",
         TRUE ~ .x # maintain existing value
       ),
       .names = "{.col}2" 
     )) %>%
     .$species_plus2 %>%
     table()
   ```




# Data visualisation

The [ggplot2](https://ggplot2.tidyverse.org/){target="_blank"} package is well known for plotting figures

## Histograms

```{r}
penguins %>%
  ggplot(aes(x = flipper_length_mm, fill = species)) + geom_histogram(bins = 40)
```

## Facet wrap

```{r}
penguins %>%
  ggplot(aes(x = flipper_length_mm, fill = sex)) +
    geom_histogram() +
    facet_wrap(~species, dir = "v", scale = "free_y")
```




# Analysis: Descriptive statistics

## Analysis: Bivariate tests for independent observations {.smaller .scrollable}

| Variable 1  | Variable 2  | Bivariate test                                                                                          |
| ----------- | ----------- | ------------------------------------------------------------------------------------------------------- |
| Categorical | Categorical | 1. Chi-square test <br> 2. Fisher's exact test                                                          |
| Categorical | Continuous  | Parametric: <br> 1. Independent t-test (2 categories) <br> 2. One-way independent ANOVA (>2 categories) |
| Categorical | Continuous  | Non-parametric: <br> 1. Mann-Whitney test (2 categories) a.k.a. Wilcoxon rank-sum test <br> 2. Kruskal-Wallis test (>2 categories) |
| Continuous  | Continuous  | Parametric: Pearson's correlation coefficient                                                           |
| Continuous  | Continuous  | Non-parametric: Spearman's correlation coefficient                                                      |

: {tbl-colwidths="[25,25,50]"}

## Analysis: Bivariate tests for repeated measurements {.smaller .scrollable}

| Variable 1  | Variable 2  | Bivariate test                                                               |
| ----------- | ----------- | ---------------------------------------------------------------------------- |
| Categorical | Categorical time <br> (e.g. baseline, 12-month) | McNemar's test                            |
| Continuous  | Categorical time <br> (e.g. baseline, 12-month) | Parametric: Dependent t-test              |
| Continuous  | Categorical time <br> (e.g. baseline, 12-month) | Non-parametric: Wilcoxon signed-rank test |

: {tbl-colwidths="[25,37.5,37.5]"}


## Measures of central tendency {.smaller .scrollable}

1. Mean (SD), median (IQR) helps you get a sense of the distribution of characteristics in your study population with respect to levels of the outcome
1. The [`tableby` function](https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html){target="_blank"} of the [`arsenal`](https://mayoverse.github.io/arsenal/){target="_blank"} package lets you do this easily
1. We will talk about improving upon the formatting of the table in [presenting tables](#pres_table)

```{r}
#| code-line-numbers: "|3,4,5"
library(arsenal)

tableby(species ~ sex + island + bill_length_mm,                                                        # <1>
        data = penguins,                                                                                # <1>
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"))) %>%     # <1>
summary(text = TRUE) %>%
knitr::kable(align = c("l", rep("r", 5)))
```

1. `tableby` function invoked by these 3 lines


## Measures of central tendency (repeated measures) {.scrollable}

::: {.r-fit-text}

1. Suppose we have repeated measures of HbA1c and LDL from patients at e.g. baseline and 12-month
   ```{r}
   generate_fake_patient_data <- function(id) {
     data.frame(list(
       patient_id = id,
       time = c("baseline", "12-month"),
       hba1c = runif(n = 2, min = 4, max = 14),
       ldl = sample(c("good control", "bad control"))
     ))
   }
   
   set.seed(2024)
   data <- map(
     sprintf("P%s", str_pad(1:30, width = 2, pad = 0)),
     generate_fake_patient_data
   ) %>%
     bind_rows() %>%
     mutate(across(.cols = time,
                   .fns = ~ fct_relevel(.x, "baseline", "12-month")))
   print(head(data, 10))
   ```

1. The [`paired` function](https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html){target="_blank"} from [`arsenal`](https://mayoverse.github.io/arsenal/){target="_blank"} package helps us tabulate pre-post tests easily

   ```{r}
   # pacman::p_load(arsenal, kableExtra)
   
   table <- paired(
     time ~ signed.rank(hba1c) + ldl,
     data = data,
     id = patient_id,
     control = paired.control(digits = 2,
                              numeric.stats = c("Nmiss2", "meansd", "medianq1q3", "range"),
                              numeric.test = "signed.rank",
                              signed.rank.correct = FALSE))

   table %>%
     summary(text = TRUE) %>%
     kable(align = c("l", rep("r", 4))) %>%
     column_spec(column = 1, width_min = "4cm") %>%
     column_spec(column = 2:4, width_min = "3cm") %>%
     column_spec(column = 5, width_min = "2.5cm")
   ```

   ```{r}
   tests(table) %>%
   kable()
   ```

:::




# Analysis: Generalised Linear Models (GLM) {#ana_glm}

## How are categorical variables represented? {.smaller}

1. Categorical variables are characters, but we need a numeric matrix in order to perform computations to get the $\beta$ coefficients
```{r}
set.seed(10)
idx <- sample(1:344, 5)
penguins[idx,]
```

1. So one-hot encoding / dummy encoding will be applied to categorical variables
```{r}
model.matrix(~species, penguins)[idx,]
```


## Framework {.smaller}

```{mermaid}
%%| echo: false
%%| fig-width: 9

flowchart LR
  A[Model specification] --> B[Inference] --> C[Diagnostics]
```

1.  Model specification
    a.  How to decide what type of model to fit?
    b.  How to decide what variables to choose?
2.  Inference
    a.  How do we get the value of the coefficients, confidence intervals, p-values?
3.  Diagnostics
    a.  Does the model fit our data well?


## Why the need for GLM? {.smaller}

1.  We want to find the association between an outcome (e.g. BMI) and some dependent variables (e.g. age, gender, ethnicity, social economic status)
2.  We are used to fitting a multivariable linear regression $$
    \begin{align}
      \underbrace{BMI}_{\text{continuous}} &= \underbrace{\beta_0 + \beta_1age + \beta_2gender + \beta_3ethnicity + \cdots + \epsilon}_{\text{this linear combination is continuous}} \\
      \epsilon &\sim N(0, \sigma^2)
    \end{align}
    $$
3.  What if your outcome is not continuous?
    i.  E.g. outcome takes on values of 0 or 1 (logistic regression)
    ii. E.g. outocme takes on discrete integer values 0, 1, 2, ... (poisson regression)
4.  GLM gives you a way to relate the outome (which can take on various distributions) with a linear combination of dependent variables

## Formal definition of GLM {.smaller .scrollable}

GLM consists of 2 components

::: {.callout-tip}
### Component 1: What **distribution** does your outcome, $y_i$ take on?

\begin{align}
    y_i|X_i \sim \text{member of exponential family} \ \text{(e.g. $y_i$ is normally distributed)}
\end{align}

i. $i$ indexes the $i$-th observation of your dataset i.e. corresponds to a particular row of your dataset
i. $y_i$ represents the $i$-th observation of your outcome variable
i. $X_i$ represents the $i$-th observation of all your independent variables (e.g. age, gender, ethnicity)

:::

::: {.callout-warning}
### Component 2: What is the **link function** $g$ you want to use?

\begin{align}
    g(\mathbb{E}(y_i|X_i)) = X_i^{T} \beta = \underbrace{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}_{\text{this linear combination is continuous}}
\end{align}

i.  $g$ is the link function that maps $\mathbb{E}(y_i|x_i)$ to $X_i^{T}\beta$
i.  It is the link function that enables the mapping of the continuous $X_i^{T}\beta$ to $y_i$, which can be discrete for example
:::

2.  In a generalised linear model, the outcome variable $y_1,...,y_n$ are modelled as independent and identically distributed (iid) observations

2.  The outcome variable is treated as a random variable (i.e. outcome takes on a certain distribution e.g. normal), but NOT the independent variables

## Family member 1: Linear regression {.smaller}

When our outcome, $y_i$ is continuous and takes on real values (i.e. $y_i \in \mathbb{R}$), we may choose to model $y_i$ with a normal distribution

::: {.callout-tip}
### Component 1: $y_i|x_i$ is normally distributed
$$
y_i | x_i \sim \mathcal{N}(X_i^{T}\beta, \sigma^2)
$$
:::

::: {.callout-warning}
### Component 2: link function is the identity function i.e. no transformation done
$$
\mathbb{E}(y_i|x_i) = X_i^{T}\beta = \beta_0 + \beta_1age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots
$$
:::

------------------------------------------------------------------------

## Running a linear regression in R {.smaller .scrollable}

1.  Notice the use of the formula syntax. LHS of the `~` is the dependent variable, while RHS are the independent variables
1.  Notice the `family` argument contains the 2 components of the GLM we discussed earlier
    ```{r}
    #| code-line-numbers: "|2|6|8"
    model <- glm(body_mass_g ~ species + bill_length_mm +
                               bill_depth_mm + 
                               flipper_length_mm + sex,
                 data = penguins,
                 family = gaussian(link = "identity"))
    
    summary(model)
    ```

1. We can call `coef` and `summary` to access the beta coefficients and p-values
   ```{r}
   coef(summary(model))
   ```

1. We can call `confint.default` to get the confidence intervals
   ```{r}
   confint.default(model)
   ```

## Tabulating the results of a linear regression {.smaller .scrollable}

1. We can make use of the of the [`tabulate_glm_result`](https://jeremylew.github.io/ezyr/reference/tabulate_glm_result.html){target="_blank"} function in the [`ezyr`](https://jeremylew.github.io/ezyr/reference/index.html){target="_blank"} package to tabulate our results

```{r}
# library(ezyr)

model %>%
  tabulate_glm_result() %>%
  mutate(across(.cols = "variable_name", 
                .fns = ~ case_when(var0 == "header" ~ sprintf("**%s**", .x),
                                   TRUE ~ sprintf("- %s", .x)))) %>%
  mutate(across(.cols = "Beta (95% CI)",
                .fns = ~ case_when(var0 == "Ref" ~ "Ref",
                                   TRUE ~ .x))) %>%
  # bold p-values if < 0.05
  mutate(across(.col = "p_value_",
                .fns = ~ case_when(p_value < 0.05 ~ sprintf("**%s**", .x),
                                   TRUE ~ .x))) %>%
  select(variable_name, "Beta (95% CI)", p_value_) %>%
  rename("Independent variables" = variable_name,
         "p-value" = p_value_) %>%
  kable(align = c("l", rep("r", 2))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                fixed_thead = T, full_width = T)
```

------------------------------------------------------------------------

## Family member 2: Logistic regression {.smaller .scrollable}

When our outcome, $y_i$, is discrete and dichotomous (i.e. take two discrete values, 0-1, success-failure), we can model $y_i$ with a Bernoulli distribution

::: {.callout-tip}
### Component 1: $y_i|x_i$ is Bernoulli distributed

$$
y_i | x_i \sim \mathrm{Bernoulli}(\pi_i)
$$

- $\pi_i$ is the probability of success of $y_i$
- $\mathbb{E}(y_i|x_i) = \pi_i$
:::

::: {.callout-warning}
### Component 2: Link function, $g$, is the logit function

\begin{align}
    g(\mathbb{E}(y_i|x_i)) &= \underbrace{\mathrm{ln} \left( \frac{\mathbb{E}(y_i|x_i)}{1-\mathbb{E}(y_i|x_i)} \right)}_{logit[\mathbb{E}(y_i|x_i)]} = X_i^{T}\beta = \underbrace{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}_{\text{this linear combination is continuous}} 
\end{align}
:::

------------------------------------------------------------------------

## Running a logistic regression in R {.smaller .scrollable}

1. Suppose we classify penguins into _light_ & _heavy_ based on body mass
1. Run logistic regression model with `body_mass_binary` as the dependent variable
1. Based on the factor levels, _light_: 0 and _heavy_: 1

   ```{r}
   # Classify penguins into "light" & "heavy"
   penguins <- penguins %>%
     mutate(body_mass_binary = if_else(body_mass_g < 4750, "light", "heavy") %>%
                               fct_relevel("light", "heavy"))
   
   # Run logistic regression model
   log_regression_model <- glm(body_mass_binary ~ species + bill_length_mm +
                                 bill_depth_mm + flipper_length_mm + sex,
                               data = penguins,
                               family = binomial(link = "logit"))
   
   summary(log_regression_model)
   ```

1. We can call `coef` and `summary` as before but to get odds ratios we must
   exponentiate the beta coefficients using `exp`
   ```{r}
   exp(coef(summary(log_regression_model)))
   ```

1. We must likewise exponentiate the confidence intervals of the beta coefficients
   to get the confidence intervals of the odds ratios
   ```{r}
   exp(confint.default(log_regression_model))
   ```

## Tabulating the results of a logistic regression {.smaller .scrollable} 

1. We can make use of the of the [`tabulate_glm_result`](https://jeremylew.github.io/ezyr/reference/tabulate_glm_result.html){target="_blank"} function in the [`ezyr`](https://jeremylew.github.io/ezyr/reference/index.html){target="_blank"} package to tabulate our results
1. Remember to set `exponentiate = TRUE` as we need to exponentiate the beta coefficients to get odds ratios

```{r}
log_regression_model %>%
  tabulate_glm_result(exponentiate = TRUE) %>%
  mutate(across(.cols = "variable_name", 
                .fns = ~case_when(var0 == "header" ~ sprintf("**%s**", .x),
                                  TRUE ~ sprintf("- %s", .x)))) %>%
  mutate(across(.cols = "exp(Beta) (exp(95% CI))",
                .fns = ~case_when(var0 == "Ref" ~ "Ref",
                                  TRUE ~ .x))) %>%
  mutate(across(.col = "p_value_",
                .fns = ~case_when(p_value < 0.05 ~ sprintf("**%s**", .x),
                                  TRUE ~ .x))) %>%
  select(variable_name, "exp(Beta) (exp(95% CI))", p_value_) %>%
  rename("Independent variables" = variable_name,
         "Odds ratio (95% CI)" = "exp(Beta) (exp(95% CI))",
         "p-value" = p_value_) %>%
  kable(align = c("l", rep("r", 2))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                fixed_thead = T, full_width = T)
```

## Inferring the coefficients {.scrollable}

::: {.r-fit-text}
1.  *Maximum likelihood estimation (MLE)* is the way to infer the beta coefficients
1. The idea behind MLE is that we want to maximise the joint probability of our observed data assuming that the data was generated according to our model i.e. we want to maximise the likelihood function $\mathcal{L}(\beta) = P(y_1, y_2, ..., y_n)$.
1. In the case of logistic regression, the likelihood function $\mathcal{L}(\beta)$ is:
    \begin{align}
        \mathcal{L}(\beta) &= P(y_1, y_2, ..., y_n) \\
        &= \prod_{i=1}^{n} P(y_i) \\
        &= \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
    \end{align}
1. Convert the likelihood to log-likehood to make it easier to work with (we can do that because log is a monotonic function)
    \begin{align}
        \mathrm{ln}\ \mathcal{L}(\beta)
        %
        &= \mathrm{ln} \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
        %
        &= \sum_{i=1}^{n} \left[ y_i\mathrm{ln}\pi_i + (1-y_i)\mathrm{ln}(1-\pi_i) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \mathrm{ln} \left( \frac{e^{\beta^{T} x_i}}{1+e^{\beta^{T} x_i}} \right) + (1-y_i) \mathrm{ln} \left( 1-\frac{e^{\beta^{T} x_i}}{1+e^{\beta^{T} x_i}} \right) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - y_i \mathrm{ln}(1+e^{\beta^{T}x_i}) + (1-y_i) \mathrm{ln} \left( \frac{1}{1+e^{\beta^{T}x_i}} \right) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - y_i \mathrm{ln}(1+e^{\beta^{T}x_i}) - (1-y_i) \mathrm{ln} (1 + e^{\beta^{T}x_i} ) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - \mathrm{ln}(1+e^{\beta^{T}x_i}) \right] \\
    \end{align}
1. In order to maximise the log-likelihood, we will use the iteratively reweighted least squares (IRLS) algorithm which is a Newton-Raphson like method
    \begin{align}
    \beta^{(t+1)} = \beta^{(t)} - \textit{H}^{-1}(\beta^{(t)}) \nabla_{\beta} \mathcal{L}(\beta^{(t)})
    \end{align}
    The idea behind this is to update $\beta$ via a series of iterations such that on every update, we are stepping in the direction of steepest ascent.
1. To use the IRLS, first we need to calculate the gradient of the log-likelihood
    \begin{align}
        \nabla_{\beta} \mathrm{ln}\ \mathcal{L}(\beta)
        &= \sum_{i=1}^{n} \left[ y_i \nabla_\beta \beta^{T}x_i - \nabla_\beta \mathrm{ln}(1+e^{\beta^{T}x_i})  \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i x_i - \frac{e^{\beta^{T}x_i}}{1+e^{\beta^{T}x_i}} x_i \right] \\
        %
        &= \sum_{i=1}^{n} (y_i - \pi_i) x_i \\
        & = X^{T}(y - \pi)
    \end{align}
1. Next, we will calculate the Hessian, $H$, of the log-likelihood by taking derivatives one more time from the gradient.
    \begin{align}
    H &= -\sum_{i=1}^{n} \pi_i (1-\pi_i) x_i x_i^{T} \\
    &= - X^{T} W X \\
    \end{align}

    where $W$ is a diagonal weight matrix given by:
    \begin{align}
        W = diag\{\pi_1 (1-\pi_1), \pi_2 (1-\pi_2), ..., \pi_n (1-\pi_n)\}
    \end{align}
1. Now that we have the components, we can substitute these back into (5) to expand it out
    \begin{align}
        \beta^{(t+1)} = \beta^{(t)} + (X^{T}W^{(t)}X)^{-1}X^{T}(y - \pi^{(t)})
    \end{align}
:::

## Interpret coefficients: logistic regression {.scrollable}

::: {.r-fit-text}
1. Step 1: Start from the equation linking outcome to indepenent variables
    \begin{align}
    \mathrm{ln}\left(\underbrace{\frac{\mathbb{E}(y_i|x_i)}{1-\mathbb{E}(y_i|x_i)}}_{odds_i}\right) = \beta_0 &+ \beta_1 age_i + \beta_2 gender^{(male)}_i \\ 

    &+ \beta_3 ethnicity^{(malay)}_i + \beta_4 ethnicity^{(indian)}_i + \cdots
    \end{align}

1. Step 2: Analyse unit changes

    i. Continuous example: 1 unit increase in age, with everything else held constant. The exponent of $\beta_1$ gives us the odds ratio for every unit increase in age.
        \begin{align}
            \mathrm{ln}\ odds_i|_{age=a+1} - \mathrm{ln}\ odds_i|_{age=a} &= \beta_1 (a+1) - \beta_1 (a) \\
            \mathrm{ln}\ \frac{odds_i|_{age=a+1}}{odds_i|_{age=a}} &= \beta_1 \\
            \frac{odds_i|_{age=a+1}}{odds_i|_{age=a}} &= e^{\beta_1}
        \end{align}
    
    i. Categorical example: compare to reference category, with everything else held constant. The exponent of $\beta_3$ gives us the odds ratio of malay compared to chinese.
        \begin{align}
        \mathrm{ln}\ odds_i|_{ethnicity=malay} - \mathrm{ln}\ odds_i|_{ethnicity=chinese} &= \beta_3 \\
        \mathrm{ln}\ \frac{odds_i|_{ethnicity=malay}}{odds_i|_{ethnicity=chinese}} &= \beta_3 \\
        \frac{odds_i|_{ethnicity=malay}}{odds_i|_{ethnicity=chinese}} &= e^{\beta_3}
        \end{align}

::: {.aside}
Notes: 

1. $odds_i$ represents the odds of "success" of the outcome, which is coded as 1

1. Reference categories do not have $\beta$ coefficient due to one-hot encoding e.g. gender: female, ethnicity: chinese
:::

:::


## Family member 3: Gamma regression {.smaller .scrollable} 

When our outcome $y_i$, is continuous and positive, we can model $y_i$ with a Gamma distribution

::: {.callout-tip}
### Component 1: $y_i$ is Gamma distributed
$$ 
y_i | X_i \sim Gamma(\alpha, \beta)
$$
:::

::: {.callout-warning}
### Component 2: One choice of the link function, $g$, is the ln function
$$
\begin{align*}
    g(\mathbb{E}(y_i|X_i)) = \mathrm{ln}\ \mathbb{E}(y_i|X_i) = \beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots \\
\end{align*} 
$$
:::

Notes:

1. The inverse of the ln function is the exponent function. Alternatively, we can also express $\mathbb{E}(y_i|x_i) = e^{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}$

1. From this expression, we can see that the inverse of the link function (i.e. exponent) maps $X_i^{T}\beta$ to $\mathbb{E}(y_i|X_i)$. And we know that this is a valid function because $\mathbb{E}(y_i|X_i)=e^{X_i^{T}\beta} > 0$


## Interpret coefficients: gamma regression {.scrollable}

::: {.r-fit-text}
1. Step 1: Start from the equation linking outcome to independent variables
    \begin{align}
    \mathrm{ln}\ \mathbb{E}(y_i|X_i) = \beta_0 &+ \beta_1 age_i + \beta_2 gender^{(male)}_i \\
    &+\beta_3 ethnicity^{(malay)}_i + \beta_4 ethnicity^{(indian)}_i + \cdots \\
    %
    \end{align}
1. Step 2: Analyse unit changes
    i. Continuous example: 1 unit increase in age, with everything else held constant. The percentage change in mean outcome for every unit increase in age can be given from $e^{\beta_1} - 1$
        \begin{align}
            \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a+1} - \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a} &= \beta_1 (a+1) - \beta_1(a) \\
            %
            \mathrm{ln}\ \frac{\mathbb{E}(y_i|X_i)|_{age=a+1}}{ \mathbb{E}(y_i|X_i)|_{age=a}} &= \beta_1 \\
            %
            \frac{\mathbb{E}(y_i|X_i)|_{age=a+1}}{ \mathbb{E}(y_i|X_i)|_{age=a}} -1 &= e^{\beta_1} - 1
        \end{align}
    i. Interaction terms example: compare vs reference category, age at some value $a$, with everything else held constant
        a. The percentage change in mean outcome for male compared to female can be given from $e^{\beta_2 + \beta_3 a} - 1$ 
        a. Notice now that this percentage change which was $\beta_2$ (if no interaction had been defined) is now modified by $\beta_3 a$ which depends on age
        \begin{align}
            \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a, gender=male} - \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a, gender=female} &= (\beta_1 a + \beta_2 + \beta_3 a) - \beta_1 a  \\
            %
            \mathrm{ln}\ \frac{\mathbb{E}(y_i|X_i)|_{age=a, gender=male}}{ \mathbb{E}(y_i|X_i)|_{age=a, gender=female}} &= \beta_2 + \beta_3 a \\
            %
            \frac{\mathbb{E}(y_i|X_i)|_{age=a, gender=male}}{ \mathbb{E}(y_i|X_i)|_{age=a, gender=female}} -1 &= e^{\beta_2 + \beta_3 a} - 1
        \end{align}
:::

## Family member 4: Conway-Maxell-Poisson (CMP) regression {.smaller}

1. When our outcome variable is a "count", the poisson regression model is used because the poisson distribution models counts better than say
linear regression which assumes that the outcome variable is normally distributed
1. However, the poisson regression model is restrictive as it assumes equi-dispersion in our outcome variable (i.e. mean of outcome variable approximately equals the variance of the outcome variable)
When there is under- or over-dispersion, the poisson distribution no longer models the outcome variable well.
1. When there is over-dispersion (variance of outcome > mean of outcome), negative binomial regression is often used
1. When there is under-dispersion (i.e. variance of outcome < mean of outcome), we will use the Conway-Maxwell-Poisson regression model
1. Useful resources: (i) Video explanations see [@videolectureschannelFlexibleModelCount2012;@consortiumObservationDrivenConwayMaxwell2018] (ii) Sellers & Premeaux [@sellersConwayMaxwellPoisson2020] explains the CMP regression and surveys available statistical packages

## CMP distribution {.smaller}

<!-- 1. Background info on the Conway-Maxwell-Poisson (CMP) distribution: -->
1. The Conway-Maxwell-Poisson distribution differs from the Poisson distribution with the introduction of a parameter $\nu$
1. The $\nu$ parameter enables the Conway-Maxwell-Poisson distribution to model a wide range of dispersion, and generalises well-known distributions
    a. When $\nu = 1$, it takes on the Poisson distribution (equi-dispersion)
    a. When $\nu = 0$ and $\lambda < 1$, it takes on the geometric distribution (over-dispersion)
    a. When $\nu \rightarrow \infty$, it takes on the bernoulli distribution (under-dispersion)

## CMP regression {.smaller}
1. Use of the Conway-Maxwell-Poisson (CMP) distribution in the generalised linear model (GLM) setting:
    i. In the original CMP formulation of Sellers and Shmueli[@sellersFlexibleRegressionModel2010], the relationship between
       dependent variable $Y$ and independent variables $X$ is given by the equations:
        a. $ln(\lambda_{i}) = X_{i}^T\beta$
        a. $\mathbb{E}(Y_{i}) = \lambda_{i} \frac{\partial ln Z(lambda_{i}, \nu)}{\partial \lambda_i} \approx \lambda_{i}^{\frac{1}{\nu}} - \frac{\nu - 1}{2\nu}$,
           where $Z(\lambda_i, \nu_i)$ is a normalizing constant
1. However, the CMP regression model is not amenable to interpretation of coefficients by mean contrasts, because $\lambda_i$ is related to $\mathbb{E}(Y_i)$ by a non-linear function
1. To overcome this, Huang 2017[@huangMeanparametrizedConwayMaxwell2017] reparameterised the CMP distribution to model the mean directly. He called it CMP~$\mu$~

## CMP~$\mu$~-regression {.smaller}

1. Under the CMP~$\mu$~-regression formulation, the relationship between dependent variable $Y$ and independent variables $X$ is given by $ln(\mathbb{E}(Y_i|X)) = X_{i}^T\beta$
1. The convenient thing about CMP-regression and CMP~$\mu$~-regression is that we can conduct a likelihood ratio test[@sellersFlexibleRegressionModel2010;@huangMeanparametrizedConwayMaxwell2017]
   to see if a poisson regression model (poisson regression is a special case of CMP-regression when $\nu = 1$) is adequate (i.e. $H_0: \nu = 1$ vs $H_1: \nu \ne 1$) 
1. Note that $H_1$ does not specify the direction (under vs over) of data dispersion. This can be assessed via the dispersion estimate $\hat{\nu}$[@sellersFlexibleRegressionModel2010]
   (i.e. $\hat{\nu} > 1$ if under-dispersion and $\hat{\nu} < 1$ if over-dispersion)
1. Model diagnostics[@huangMeanparametrizedConwayMaxwell2017]: If the underlying distributional model is correct, the probability inverse transformation (PIT) should resemble
   a random sample from a standard uniform distribution. Goodness-of-fit can then be assessed by plotting a histogram of the PIT, or via a quantile plot of the PIT against
   the uniform distribution
    
## Running a CMP~$\mu$~-regression in R

We will use the [mpcmp](https://thomas-fung.github.io/mpcmp/){target="_blank"}[@fungMpcmpMeanParametriziedConwayMaxwell2020] R package's implementation of the CMP~$\mu$~-regression


## Different meaning under different contexts {.smaller}

1. We can use a generalised linear model under different contexts
   i. Descriptive: to reason about association(s) between independent variable(s) and dependent variable. This is the context we are operating under when we use e.g. a multivariable linear regression model in a cross sectional study
   i. Causal inference: to reason about the causal effect of one variable on another
   i. Prediction: to predict the dependent variable given new observations of independent variables

1. Under different contexts
   i. The assumptions made are different
   i. The purpose of the model are different
   i. The conclusions which you draw from the model are different

## Use of a GLM in the causal inference context

::: {.r-fit-text}

1. We refer readers to the work of Judea Pearl[@pearlCausalInferenceStatistics2016]
1. An example of a study in healthcare where GLM is used to infer causal associations from observational data is by Laura et al.[@lauraDepressiveSymptomsMalnutrition2022]
1. A causal diagram which diagrams out the causal relationships between variables, in the form of a directed acyclic graph, is first posited
1. Supposing that the causal relationships as expressed in the causal diagram were true, it proceeds that we can infer causal association between a variable of interest and the outcome by applying the theory e.g. back-door criterion, front-door criterion. Therefore, the variables of the regression model are chosen in accordance with the causal diagram e.g. to block all back-door paths between a variable of interest and the outcome.

:::

## Use of the GLM in the descriptive context {.scrollable}

::: {.r-fit-text}

1. My hypothesis is that before the technology of causal inference came about, people know of the problem of confounding in observational data. However, prior to the theory of causal inference, confounding was not well defined. There was no theoretical foundation that lays out how to deal with confounding in a mathematical precise way.
1. In the context of using GLM (e.g. multivariable linear/logistic regression) in cross-sectional observational data, the GLM then was a means to reason/discuss about association, but we can only reason about association in the form of comparative statements of means and odds in the descriptive sense (examples are in the following slides) rather than causal associations. This is because we neither posited a causal diagram, nor did we identify a variable of interest and the back-door paths with the outcome. Instead we are looking if there are any statistically significant association among all independent variables chosen.
1. When it comes to model specification (i.e. choosing the variables in the model), we would select all measurable variables (or proxy variables for unmeasurable ones) which we hypothesize from our domain clinical knowledge or literature review to have a causal relationship with the dependent/outcome variable (even though it was not set up for causal inference)
1. Because the model is used for a descriptive purpose, having more variables (as your sample size allows for) that are of causal interest makes for a more informative descriptive study, simply for the reason that you have considered more variables that are of causal interest.

:::

## Logistic regression in the descriptive context {.scrollable}

::: {.r-fit-text}

1. Under the descriptive context, an example of the kind of comparative statements we make with the logistic regression model are as follows
1. Categorical variable (e.g. age):
   i. Patients aged < 40 years old (one of the categories of the categorical age variable) are more likely to be depressed (the category of the outcome/dependent variable coded as 1) compared to patients aged > 65 years old (reference category).
   i. On average, if we were to compare a patient aged < 40 years old against another patient aged > 65 years old, where they only differ in age but who otherwise are same in the other variables in the model (i.e. same gender, same number of conditions etc.), then the odds of the patient aged < 40 years old being depressed is 3 times the odds of the patient aged > 65 years old being depressed.
1. Continuous variable (e.g. EQ5D score):
   i. Patients who score higher on the EQ5D utility index are less likely to be depressed compared to patients who score lower on the EQ5D index.
   i. On average, if we were to compare patient A with EQ5D score of $x$ + 1 (where $x$ is any given value for EQ5D score) against patient B who has an EQ5D score of $x$, where patient A and B only differ by 1 unit of EQ5D score but who otherwise are same in the other variables in the model, then the odds of patient A being depressed is 0.02 times the odds of patient B being depressed.
   i. Or we can also say that, for every increase of 1 point in EQ5D score, the odds of a patient being depressed decreases by 98% when controlling for other variables
1. The odds ratio is statistically significantly different (or not) from 1.

:::

## Linear regression in the descriptive context

::: {.r-fit-text}

1. Under the descriptive context, an example of the kind of comparative statements we make with the linear regression model are as follows
1. Categorical variable (e.g. age): On average, if we were to compare a patient aged < 40 years old against another patient aged > 65 years old (reference category), where they only differ in age category but who otherwise are same in the other variables in the model (i.e. same gender, same number of conditions etc.), then the PHQ9 score (dependent variable) of the patient aged < 40 years old is on average 5 units higher (beta coefficient of 5, a unit could be a point, 10 points etc.) than that of the patient aged > 65 years old.
1. Continuous variable (e.g. EQ5D score): On average, if we were to compare patient A with EQ5D score of $x$ + 1 (where $x$ is any given value for EQ5D score) against patient B who has an EQ5D score of $x$, where patient A and B only differ by 1 unit of EQ5D score but who otherwise are same in the other variables in the model, then the EQ5D score of patient A is on average 3 units lower (beta coefficient of -3) than that of patient B.
1. The beta coefficient is statistically significantly different (or not) from 0.

:::

## Use of the GLM in the prediction context

::: {.r-fit-text}

1. Aim is to fit a model that learns the correlation between dependent and independent variables on a "training" dataset which would then allow us to make our prediction on unseen "test data"
1. Assumption here is that the data distribution of the test data is similar to the training data
1. Here we are more concerned about selecting informative variables or creating variable(s) from transformation(s) of existing variables such that the model is able to capture the correlation between independent and dependent variables better, and hence achieve better predictive performance
1. We are less concerned about statistical significance per se, but we may use it as a variable selection method in so far as it gives us better predictive performance

:::


# Analysis: Generalised linear mixed models

## Mixed effects multinomial logistic regression model {.smaller .scrollable}

The random intercept model is given as follows:

$$
\begin{align*}
&ln \left(\frac{\pi_{2ij}}{\pi_{1ij}} \right) = \beta_{02} + \beta_{12}x_{ij} + u_{2j} \\
&ln \left(\frac{\pi_{3ij}}{\pi_{1ij}} \right) = \beta_{03} + \beta_{13}x_{ij} + u_{3j} \\
& \pi_{1ij} + \pi_{2ij} + \pi_{3ij} = 1
\end{align*}
$$

$$
\begin{bmatrix} 
u_{2j} \\
u_{3j} \\
\end{bmatrix}
\sim N \left( 
\begin{bmatrix}
0 \\
0 \\
\end{bmatrix}
,
\begin{pmatrix}
  \sigma^2_{u2} & \\
  \sigma_{u23} & \sigma^2_{u3} \\
\end{pmatrix} 
\right)
$$

Where:

1. $y$ represents the outcome variable
1. $x$ represents the independent variable
1. $i$ represents the $i$-th observation in our dataset of e.g. N = 1,000
1. $j$ represents the $j$-th group e.g. $J$ = 7 polyclinics
1. $u_{2j}$ and $u_{3j}$ are the random intercepts corresponding to the $j$-th polyclinic
1. $\pi_{1ij} = P(y_{ij} = 1)$, where 1 refers to the reference category of $y$
1. $\pi_{2ij} = P(y_{ij} = 2)$, where 2 refers to the second category of $y$ 
1. $\pi_{3ij} = P(y_{ij} = 3)$, where 3 refers to the third category of $y$




# Analysis: Concordance

## What is the Kappa statistic?

1.  The Kappa statistic is a measure of "true" agreement. It indicates the proportion of agreement beyond that expected by chance [@simKappaStatisticReliability2005]
2.  If prevalence index is high, chance agreement will be higher, kappa is reduced
3.  If bias index is high, chance agreement will be lower, kappa is higher

## Calculation of Kappa statistic

1.  Calculation of prevalence-adjusted, bias-adjusted Kappa can be done through the [`epiR`](https://cran.r-project.org/web/packages/epiR/index.html){target="_blank"} package

<!-- ## Sample size calculation -->




# Analysis: Tests of reliability

## Internal Consistency

::: {.r-fit-text}

1. Cronbach's alpha is a measure of internal consistency of questionnaire
1. Interpreting Cronbach’s alpha
    i. Cronbach’s alpha between 0.70 to 0.80 is considered good enough[@fieldDiscoveringStatisticsUsing2012]
    i. Cronbach’s alpha is shown as raw_alpha in the output
1. Caveats:
    i. All else equal, alpha will increase as the number of items on the scale increases[@fieldDiscoveringStatisticsUsing2012] (page 799, section 17.8.2)
    i. If responses lack variability (i.e. almost everyone gives the same response), alpha tends to be poor[@fieldDiscoveringStatisticsUsing2012] (page 803)
    i. "If alpha is too high it may suggest that some items are redundant as they are testing the same question but in a different guise. A maximum alpha value of 0.90 has been recommended" [@tavakolMakingSenseCronbach2011]

:::

## Test-retest reliability

::: {.r-fit-text}

1. We will use the Intraclass Correlation Coefficient to assess for test-retest reliability
1. Out of the 10 variants of ICC, we will compute the "two-way mixed effects, absolute agreement, single rater/measurement" ICC variant or ICC (A,1) of McGraw and Wong[@mcgrawFormingInferencesIntraclass1996] per recommendations from[@kooGuidelineSelectingReporting2016; @qinAssessingTestretestReliability2019]:
1. When it comes to interpreting the ICC, according to Koo et al.[@kooGuidelineSelectingReporting2016]:
    i. ICC less than 0.5: poor reliability
    i. ICC between 0.5 and 0.75: moderate reliability
    i. ICC between 0.75 and 0.90: good reliability
    i. ICC greater than 0.90: excellent reliability

:::




# Analysis: Comparative effectiveness research using observational data

## What is comparative effectiveness research? {.smaller}

1. Comparative effectiveness research is the generation and synthesis of evidence that compares the benefits and harms of alternative methods to prevent, diagnose, treat, and monitor a clinical condition or to improve the delivery of care [@gatsonisMethodsComparativeEffectiveness2015]
1. _Effectiveness_ vs _efficacy_
    i. _Effectiveness_ refers to the performance of an intervention in "real-world" clinical settings while _efficacy_ refers to the performance of an intervention under "ideal" circumstances [@gatsonisMethodsComparativeEffectiveness2015]
1. Examples:
    i. Compare the effectiveness of different treatment options [@jaksaUsingPrimaryCare2022]
    i. Compare the effectiveness of a health intervention programme vs usual care group [@luImpactLongitudinalVirtual2021]


## Target trial framework {.smaller .scrollable}

1. Suppose you are interested in conducting a pragmatic randomised controlled trial (RCT) to evaluate your health intervention vs usual care
1. However, conducting this trial is infeasible so we resort to emulating this trial with observational data
1. The target trial framework[@hernanUsingBigData2016; @hernanSpecifyingTargetTrial2016] gives us guidelines to conceptualise our study
1. An example of the application of the target trial framework[@coteIntroductionTargetTrial2024]
![Table III from Cote et al.](images/cote_2024_table3.png)


## Target trial framework pointers {.smaller .scrollable}

1. Extra pointers adapted from [@hernanUsingBigData2016; @wangTargetTrialFramework2024]

<table>
  <colgroup>
    <col style="width:20%">
    <col style="width:80%">
  </colgroup>
  <thead>
    <tr>
      <th>Protocol component</th>
      <th>Observational study that
          emulates the RCT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Eligibility criteria</td>
      <td>
        • Apply the same inclusion/exclusion criteria<br>
        • Assuming that we have variables needed for inclusion/exclusion in our observational dataset<br>
        • Eligibility should only be determined based on info available at baseline (cf. immortal time bias)<br>
        • Selection bias if we exclude patients with missing data (analogue of dropout/loss to follow-up in a trial) if dropout is associated with intervention and outcome<br>
        • Patients could be eligible at multiple time points. We have a choice of (1) single eligible time: set baseline as the first timepoint when eligibility is met within a time period or (2) all eligible times (or large subset): emulate multiple nested trials
      </td>
    </tr>
    <tr>
      <td>Treatment strategies</td>
      <td>
        • Assign patients to strategy consistent with their baseline data<br>
        • Ensure treatment arms are consistent e.g. both arms contain only new initiators of a treatment as opposed to one arm having patients who are already receiving treatment some time before baseline, which affects the outcome<br>
        • Positivity: _"each individual should have positive probability of receving each level of exposure for every combination of covariates"_
          (i.e. a patient should not be precluded from a level of exposure to the intervention at the outset)<br>
        • Consistency: _"there cannot be two versions of a treatment that would result in different outcomes"_
      </td>
    </tr>
    <tr>
      <td>Assignment procedures</td>
      <td>
        • Can only emulate trials without blind assignment because individuals/care providers in the dataset
          are aware of the received treatments<br>
        • Patients are not randomly assigned to treatment strategies. methods to achieve exchangeability at baseline e.g. propensity score matching, inverse probability weighting and g-methods to deal with time-varying confounding<br>
        • Limitation: residual confounding from unmeasured confounders
      </td>
    </tr>
    <tr>
      <td>Follow-up period</td>
      <td> </td>
    </tr>
    <tr>
      <td>Outcome</td>
      <td> </td>
    </tr>
    <tr>
      <td>Causal contrast of interests</td>
      <td>
        • Intention-to-treat effect: we would use patients who were prescribed medication for treatment initiation<br>
        • Per-protocol effect: we would use patients who adhered to the treatment. Adjustment for postbaseline confounding is needed because postbaseline prognostic factors associated with subsequent adherence to treatment may be affected by prior adherence
      </td>
    </tr>
    <tr>
      <td>Analysis plan</td>
      <td>
        • Proper definition of baseline/time zero is important (affects assessment of eligibility)<br>
        • Best way to define time zero is the time time when an eligible individual initiates a treatment strategy
      </td>
    </tr>
  </tbody>
</table>


## Propensity score matching {.smaller}

1. What is the purpose of propensity score matching?

   - The purpose is to find a control group which is comparable to the intervention group from observational data
   - We want the control and intervention groups to be comparable in the sense that they have similar covariate distributions

1. What is a propensity score?
   - The propensity score, $e_i$, for individual $i$ is the conditional probability of receiving the treatment/intervention (i.e. $T_i = 1$) given a set of observed covariates $X_i$ or $e_i(X_i) = P(T_i = 1 | X_i)$

## How is propensity score matching done?

::: {.r-fit-text}

1. How is propensity score estimated?

   i. The propensity score can be estimated using any model that models the relationship between a binary outcome and a set of covariates
   i. The most commonly used model is the logistic regression model

1. How is the matching done?
   i. There are various methods of matching (e.g. nearest neighbour, optimal)
   i. Most common matching method is nearest neighbour matching

:::

## Propensity score matching in R

::: {.r-fit-text}

1. We can use the [MatchIt](https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html){target="_blank"} package to conduct propensity score matching
1. The MatchIt package implements a variety of [matching methods](https://cran.r-project.org/web/packages/MatchIt/vignettes/matching-methods.html){target="_blank"} that we can choose from

```{r}
library(MatchIt)

# Set seed for reproducibility
set.seed(1234)

# Set row names as ID column
lalonde <- lalonde %>% rownames_to_column("ID")

# Conduct propensity score matching
match_obj <- matchit(
  treat ~ age + educ + race + married + nodegree + re78,
  data = lalonde,
  method = "nearest", # nearest neighbour matching
  distance = "glm", link = "logit", # default is logistic regression
  replace = FALSE, # no replacement, each control unit can only be matched to one treated unit
  ratio = 1 # 1:1 matching
)

print(match_obj)
```

:::

## How do we evaluate covariate balance after matching? {.smaller}

1. We make use of the absolute _standardized difference_ metric
1. Rule of thumb

## Evaluating covariate balance in R {.smaller .scrollable}

```{r}
# Get matched subjects
matched_ids <- match.data(match_obj)$ID
lalonde_matched <- lalonde %>% filter(ID %in% matched_ids)

tableby(treat ~ age + educ + race + married + nodegree + re78,
        data = lalonde,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2, numeric.test = "kwt",
                                  total = FALSE, test = FALSE)) %>%
extend_tableby(lalonde, smd = TRUE, smd_digits = 2) %>%
safe_left_join(
  tableby(treat ~ age + educ + race + married + nodegree + re78,
          data = lalonde_matched,
          control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                    digits = 2, numeric.test = "kwt",
                                    total = FALSE, test = FALSE)) %>%
  extend_tableby(lalonde_matched, smd = TRUE, smd_digits = 2) %>%
  select(-Variable),
  by = "marker") %>%
select(-marker) %>%
kable(align = c("l", rep("r", 6))) %>%
column_spec(column = 1, width_min = "4.75cm") %>%
column_spec(column = c(2, 3, 5, 6), width_min = "4.5cm") %>%
column_spec(column = c(4, 7), width_min = "2.5cm") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), fixed_thead = T, full_width = T) %>%
add_header_above(c(" " = 1, "Before matching" = 3, "After matching" = 3))
```

## How do we choose matching variables?


## Why use standardised difference and not p-value? {.smaller}

::: {.r-fit-text}

The use of standardised difference to evaluate covariate balance is preferred over p-values from statistical tests (e.g. t-test, chi-square test) for the reasons as follows

1. A p-value from a statistical test is affected by sample size while standardised difference is not[@stuartMatchingMethodsCausal2010a; @austinBalanceDiagnosticsComparing2009]
	 i. If the sample size is small and hence we have a lack of power, we may get a non-significant p-value (suggesting balance) due to the lack of power, even though the groups are actually imbalanced
	 i. Conversely, if the sample size is large, we may see a significant p-value due to high power (suggesting imbalance), when the groups are effectively balanced with very small differences between the groups
1. Covariate balance should be a property of the data sample without any reference to the population from which we obtain the data[@stuartMatchingMethodsCausal2010a]
	 i. The standardised difference directly assesses balance between samples by comparing (i) means in relation to standard deviation for continuous variables, and (ii) proportions for categorical variables
	 i. But with p-values we are making inferential statements about the population from which we sample our data

:::

## Considerations of M:1 matching {.scrollable}

::: {.r-fit-text}

1. What are some considerations when we conduct many-to-one ($M:1$) matching? i.e. match multiple control patients to each intervention group patient
1. There is a bias-variance trade-off going on when we vary $M$
1. On the one hand, poorer covariate balance between treatment and control group at baseline is observed when we increase $M$
	 i. This is because matching quality drops as $M$ is increased e.g. if $M = 4$, the fourth control matched to a treatment group patient might not be as close in propensity score compared to the first/second control matched to that same treatment group patient
	 i. If we do have imbalanced treatment and control groups at baseline, then the treatment effect estimate would be biased (i.e. higher or lower than the true value) due to confounding. So if we match with $M > 1$, we just have to make sure that we achieve adequate balance in our treatment and control groups at baseline
1. On the other hand, increasing $M$ (and hence increasing the sample size of the control group) increases the precision (i.e. decreases sampling variability) of the treatment effect estimate
   i. This shows up as narrower 95% confidence intervals of the treatment effect estimate
	 i. The 95% confidence interval means that if you were to repeat your exercise (i.e. collect data again in the same way, do the matching and estimate the treatment effect) multiple times, 95 out of the 100 times, your treatment effect estimate will fall within the given confidence interval
	 i. A narrower confidence interval means that your treatment effect estimate is more precise (i.e. less variable across repeats of your exercise)
	 i. Thus with a narrower 95% CI, you would be more confident in your treatment effect estimate, as opposed to if the 95% CI were wider which would mean that the treatment effect could take on a wide range of values across different repeats of your exercise

:::

# Analysis: Intervention effect estimation



# Analysis: Missing data

## Outline

1. Why do we care about missing data?
1. What are the types of missingness mechanisms?
1. What are the implications of each missingness mechanism?
1. Multiple imputation as a method for imputing missing data

## Motivation: why do we care about missing data? {.smaller}

1. Commonly used statistical models (e.g. GLM) require complete-cases $\rightarrow$ forced to discard data points
1. Data is expensive to collect
1. In throwing out data, you will face the problem of:
    a. Efficiency: number of data points are reduced $\rightarrow$ lose statistical power
    a. Bias: you might systematically exclude a population group and end up with a biased dataset
    a. Both bias and efficiency will influence your final conclusion

## Missingness mechanism

1. A <ins>missingness mechanism</ins> refers to the underlying process which generated the missing data
1. There are 3 missingness mechanisms
    i. Missing completely at random (MCAR)
    i. Missing at random (MAR)
    i. Missing not at random (MNAR)

## Framework for understanding missingness mechanism

1. We will use the framework as laid out in Mohan & Pearl[@mohanGraphicalModelsProcessing2021]
1. Reason: Clearest way to discriminate between MCAR, MAR, MNAR
1. Prerequisite: This requires an understanding of the concept of d-separation in graphical models

## Framework notation {.smaller .scrollable}

Suppose we have data in the following format[@mohanGraphicalModelsProcessing2021]
![](images/mohan_pearl_table2.png)

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1b.png){fig-align="center"}

::: {.r-fit-text}
- $A$: Age
- $G$: Gender
- $O$: Obesity \text{(partially observed)}
- $O^*$: Obesity \text{(fully observed)}
- $R_O$: Missing indicator
:::
:::

::: {.column width="75%"}
1. Fully observed variables are shaded (e.g. $A$, $G$)
1. Partially observed variables (with missing data) are non-shaded (e.g. $O$)
1. To model the missingness process, two variables are introduced:
    i. $R_O$ - A binary {0, 1} masking variable that governs missingness
    i. $O^*$ - A proxy variable for obesity that is fully observed
    $$ O^* = f(R_O, O) = 
        \begin{cases}
            O  & \text{if } R_O = 0 \\
            m  & \text{if } R_O = 1
        \end{cases} $$
:::
:::

## #1 Missing completely at random (MCAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1b.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing obesity data is MCAR if the cause of missingness is independent of any other variable (There are no edges between $R_O$ and $G, A, O$)
1. Examples of how Obesity could be MCAR:
    i. Technical fault: eg. random connectivity downtime in the weighing machine that prevents the sending of patient’s weight to EMR
    i. Workflow: patient forgets to take his/her weight after questionnaire (and we could not call the patient to get his/her weight after)
:::
:::

### Implications of MCAR

1. Complete-case analysis $\rightarrow$ <ins>unbiased</ins> dataset, though loss of statistical power due to fewer $N$
1. Multiple imputation $\rightarrow$ <ins>unbiased</ins> dataset
    i. Multiple imputation attempts to impute missing variables using a model of the partially observed variable ($Y$) on the observed variables ($X$)
    i. Why unbiased? $R_O \perp\!\!\!\!\!\!\perp O$ implies that $P(O | A, G, R_O = 1) = P(O | A, G, R_O = 0)$
    i. The distribution of missing obesity values (which we are trying to impute) is the same as the distribution of observed obesity values, given the other observed variables like age & gender
    i. So we can estimate the missing obesity values using a logistic regression model

## #2 Missing at random (MAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1c.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing Obesity data is MAR if the cause of missingness is dependent on fully observed variable(s) eg. Age
1. Conditional independence assertion:
    i. Missingness, $R_O$, is conditionally independent of Obesity given Age i.e. $R_O \perp\!\!\!\!\!\!\perp O | A$
    i. Or you can think of it as: missingness, $R_O$, is only dependent on Age
1. Example of how Obesity could be MAR:
    i. A lot of missing weight data among teenagers who were rebellious and did not report their weight
:::
:::

### Implications of MAR

1. Complete-case analysis $\rightarrow$ <ins>biased</ins> dataset 
    i. Why biased? Data from teenage patients is excluded
1. Multiple imputation $\rightarrow$ <ins>unbiased</ins> dataset
    i. Why unbiased? Data from teenage patients is imputed and not excluded
    i. The conditional independence assertion above implies that $P(O | A, G, R_O = 1) = P(O | A, G, R_O = 0)$
    i. We can impute the missing data as the distribution of missing obesity values (which we are trying to impute) is the same as the distribution of observed obesity values, given the other observed variables like age & gender

## #3 Missing not at random (MNAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1d.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing Obesity data is MNAR if the cause of missingness is dependent on the Obesity variable itself
1. Example of how Obesity could be MNAR:
    i. A lot of missing weight data from obese patients who are embarrassed and did not disclose their weight 
    i. In another setting, we often find that low/high income earners do not disclose their income
:::
:::

### Implications of MNAR

1. Complete case analysis $\rightarrow$ <ins>biased</ins> dataset
    i. Why biased? Data from obese patients is excluded
1. Multiple imputation $\rightarrow$ <ins>biased</ins> dataset
    i. Why biased? The distribution of missing obesity values (which we are trying to impute) is different from the distribution of observed obesity values, given the other observed variables like age & gender i.e. $P(O | A, G, R_O = 1) \ne P(O | A, G, R_O = 0)$
    i. Imputing missing weight data for obese patients using data from patients who are not obese $\rightarrow$ imputed weight tend to be lower than what it should actually be
1. Imputation of missing data requires causal modelling of $R$ variables. No pure statistical method of imputation exists[@mohanGraphicalModelsProcessing2021]

## Can these missingness mechanisms be tested for? {.smaller}

1. MAR and MNAR cannot be tested for (see section 1.4 of Enders[@endersAppliedMissingData2022])
    1. In each case, an independence assertion regarding the missing variable was made
    1. Since the missing variable is by definition partially unobserved, we cannot do a test for such an assertion where the input to the test is unobserved
1. MCAR can be tested for (see section 1.9 of Enders[@endersAppliedMissingData2010])
    1. There are testable propositions, the idea is to assume that the cases with missing data came from the same population and thus have the same means and covariances as the complete cases
    1. Little's test is one test for MCAR that evaluates the mean differences across subgroups of cases that share the same missing data pattern
    1. The null hypothesis for Little's test states that the data are MCAR, so a statistically significant test statistic provides evidence against the MCAR mechanism

## Caveats about Little's test {.smaller}

> From section 1.9 of Enders[@endersAppliedMissingData2010]
>
> 1. The test does not identify the specific variables that violate MCAR, so it is only useful for testing an omnibus hypothesis that is unlikely to hold in the first place
> 1. The version of the test outlined above assumes that the missing data patterns share a common covariance matrix. MAR and MNAR mechanisms can produce missing data patterns with different variances and covariances, and the test statistic in Equation 1.4 would not necessarily detect covariance based deviations from MCAR
> 1. Simulation studies suggest that Little’s test suffers from low power, particularly when the number of variables that violate MCAR is small, the relationship between the data and missingness is weak, or the data are MNAR. Consequently, the test has a propensity to produce Type II errors and can lead to a false sense of security about the missing data mechanism

## Multiple imputation by chained equations (MICE)

## Conclusion

1. We have learnt about the 3 missingness mechanisms of MCAR, MAR, MNAR
1. We have considered the implications of *complete-case analysis* and *multiple imputation* on bias & efficiency
1. Challenge remains: a thorough handling of missing data requires causal modelling in order to:
    i. Distinguish between the 3 missingness mechanisms
    i. Impute missing data in MNAR scenario where common methods like complete-case analysis or multiple imputation are invalid


# Presentation of results {#presentation}

## Markdown

1. Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world’s most popular markup languages.[^3]
1. Try out the [markdown live preview](https://markdownlivepreview.com/){target="_blank"}

[^3]: From [Markdown Guide](https://www.markdownguide.org/getting-started/){target="_blank"}


## R Markdown {.smaller}

1. R Markdown is an extension to markdown that enables user to execute R code and display the code output in addition to text
1. The [knitr](https://yihui.org/knitr/){target="_blank"} application enables us to execute R code and display output in a temporary markdown document
1. [Pandoc](https://pandoc.org/){target="_blank"} then converts the markdown document to the desired format (e.g. html, pdf, docx, pptx)
1. See summary by [Robin Linacre](https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown){target="_blank"}

```{r}
#| echo: false
#| out-width: "40%" 
#| fig-cap: Image from [R Markdown cookbook chapter 2.1](https://bookdown.org/yihui/rmarkdown-cookbook/rmarkdown-process.html){target="_blank"}
#| fig-align: left
knitr::include_graphics(here("images/rmarkdown_knitr_process.png"))
```


## [DEMO] Creation of an R markdown document {.smaller .scrollable}

1.  Create an rmarkdown document

    ![](images/create_rmarkdown.gif){width="70%"}

1.  Knit your document into html

    ![](images/knit_rmarkdown.gif){width="70%"}


## Components of Rmarkdown document {.smaller .scrollable}

1. YAML header: controls the meta data e.g. Title, date, output document format, table of contents
1. Formatted text of your descriptions, explanations etc.
1. Code chunks: where you run R code e.g. plot graphs and display the output in the same document

::: {style="text-align:center;"}
```{r}
#| echo: false
#| out-height: "120%"
#| out-width: "120%" 
#| fig-cap: Image taken from [An Introduction to R](https://intro2r.com/r-markdown-anatomy.html){target="_blank"}
#| fig-align: "center" 
knitr::include_graphics(here("images/rmd_anatomy.png"))
```
:::


## R markdown YAML header {.smaller}

```{r}
#| echo: true
#| eval: false
---
title: "My project"
author: "CRU"
date: '`r format(Sys.Date(), "%d %b %Y")`'
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
bibliography: references.bib
csl: vancouver-superscript.csl 
link-citations: true
---
```

1. This is a common setting which I use 
1. [bookdown::html_document2](https://bookdown.org/yihui/bookdown/a-single-document.html){target="_blank"} of the [Bookdown](https://pkgs.rstudio.com/bookdown/){target="_blank"} package is a good output format because it takes care of formatting e.g. auto-numbering of sections, handling of references, appendix
1. toc (line 7 & 8) refers to table of contents
1. bibliography, csl and link-citations is for adding citations stored in a bibtex file into your document. More will be covered in a later slide


## Consort diagram {.smaller .scrollable}

1.  The [`consort`](https://cran.r-project.org/web/packages/consort/index.html) package provides a convenient `consort_plot` function to plot CONSORT diagrams for reporting inclusion/exclusion/allocation (see the [vignette](https://cran.r-project.org/web/packages/consort/vignettes/consort_diagram.html))

    ```{r}
    #| out-height: "100%"
    #| out-width: "120%"
    library(consort)
    flow_diagram <- consort_plot(data = penguins %>%
                                          mutate(id = row_number(),
                                                 exclusion = case_when(island == "Dream" ~ "Penguins on Dream island",
                                                                       year == 2007 ~ "Data collected in 2007",
                                                                       TRUE ~ NA_character_) %>%
                                                             fct_relevel("Penguins on Dream island",
                                                                         "Data collected in 2007")),
                                 orders = c(id = "Study population",
                                            exclusion = "Excluded",
                                            id = "Data for analysis"),
                                            side_box = "exclusion",
                                 cex = 1.2)
    plot(flow_diagram)
    ```

1.  For presenting in html documents, the plot function does not render the plot fully so when that happens, save it as an image first

    ```{r}
    #| eval: false
    # Change the file path to where you want your image to be saved
    ggsave(here("images/consort_diagram.png"), plot = build_grid(flow_diagram))
    ```

1.  Next, load your image using `knitr::include_graphics`

    ```{r, fig.height = 2}
    #| eval: false
    # You can use the fig.height or fig.width chunk option to rescale your image
    knitr::include_graphics(here("images/consort_diagram.png"))
    ```


## Presenting tables {#pres_table .scrollable}

::: {.r-fit-text}

```{r}
#| code-line-numbers: "|8"
pacman::p_load(arsenal, knitr)

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE) %>%
knitr::kable(align = c("l", rep("r", 5))) # <1>
```

1. The [`kable`](https://bookdown.org/yihui/rmarkdown-cookbook/kable.html){target="_blank"} function converts the table object to html

:::


## Formatting HTML tables {.scrollable}

The [kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html){target="_blank"} package provides useful functions to format tables e.g. changing column width, changing colors, font size, fixed header rows etc.

::: {.r-fit-text}

```{r}
#| code-line-numbers: "|9-11"
pacman::p_load(arsenal, knitr, kableExtra)

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE) %>%
kable(align = c("l", rep("r", 5))) %>%
kableExtra::column_spec(column = 1, width_min = "4.75cm") %>%
kableExtra::column_spec(column = 2:5, width_min = "5.75cm") %>%
kableExtra::column_spec(column = 6, width_min = "2.25cm")
```

:::


## Formatting HTML table subheaders {.scrollable}

::: {.r-fit-text}

If you added labels to your dataframe using the [`labelled`](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html){target="_blank"} package, it can be presented as subheaders via the `labelTranslations` argument in `summary`

```{r}
#| code-line-numbers: "1-4||10"
labels <- list(sex = "Sex",
               island = "Island",
               bill_length_mm = "Bill length (mm)")
labelled::var_label(penguins) <- labels

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE, labelTranslations = map(labelled::var_label(penguins), ~ str_glue("**{.x}**"))) %>% # <1>
kable(align = c("l", rep("r", 5))) %>%
kableExtra::column_spec(column = 1, width_min = "5.5cm") %>%
kableExtra::column_spec(column = 2:5, width_min = "5.75cm") %>%
kableExtra::column_spec(column = 6, width_min = "2.25cm")
```

1. The ** will render the text bold in Rmarkdown html documents

:::


## Cross-references & citations

1. See [cross-references](https://bookdown.org/yihui/bookdown/cross-references.html){target="_blank"} and [citations](https://bookdown.org/yihui/bookdown/citations.html){target="_blank"} on how to add internal cross-references and citations from a bibtex file
1. See [bibliographies and citations](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html){target="_blank"} on how to add a references and appendix section


## Reflection questions {.scrollable}

::: {.r-fit-text}

-	Can you provide a brief overview of your research and its main objectives?
-	What was your rationale for choosing this particular research topic?
-	How did you develop your research questions or hypotheses?
- What gaps in the literature is your study intended to address?
-	Can you explain your chosen methodology and why it was appropriate for your study?
-	What were the main challenges you encountered during your research, and how did you overcome them?
-	How does your research contribute to the existing body of knowledge in your field?
-	Can you discuss the key theories or concepts that informed your research?
-	Were there any conflicting views in the literature, and how did you address them?
-	How did you ensure that your literature review was comprehensive and up-to-date?
-	Can you explain your data collection process and any ethical considerations?
-	What were the main findings of your research?
-	How did you analyze your data, and why did you choose those specific analytical methods?
- How did you choose the variables in your model?
-	Were there any unexpected results, and how did you interpret them?
-	How do your findings relate to previous research in the field?
-	What are the practical implications of your research?
-	How might your findings be applied in real-world settings?
-	What are the limitations of your study, and how might they be addressed in future research?
-	If you were to continue this research, what would be your next steps?
-	Looking back, what would you do differently if you were to conduct this research again?
-	How has this research process contributed to your development as a researcher?
-	Can you identify any potential biases in your research and how you addressed them?
-	How do you see your research fitting into the broader context of your field?
-	Can you elaborate on any assumptions you made during your research?
-	How confident are you in your conclusions, and what evidence supports them?

:::



# References {.smaller}
