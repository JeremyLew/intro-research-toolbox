---
title: "Intro to research toolbox"
author: Jeremy Lew
date: last-modified
date-format: long
format:
  revealjs:
    slide-number: true
    embed-resources: true
    template-partials:
      - title-slide.html
execute:
  echo: true
bibliography: references.bib
csl: vancouver-superscript.csl 
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, magrittr, here, palmerpenguins, kableExtra)
```

## Topics

1.  Introduction & learning plan
2.  Getting comfortable with R
3.  Programming fundamentals
4.  Data cleaning
5.  Data visualisation
6.  Statistical analysis
7.  Presentation of results


# Introduction & learning plan

## Motivation

1.  Added tools to your toolbox
2.  Transferrable skill
3.  Learning how to program in 1 language helps you learn other languages
4.  Leverage on open source tools/projects written by others

## Learning plan {.smaller}

1.  The topics in this deck mirror the workflow of a project

```{mermaid}
%%| echo: false
%%| fig-width: 10

flowchart LR
  D[Data exported\nfrom REDCAP]-->C[Data cleaning]
  C-->S[Statistical analysis]
  S-->P[Presentation\nof results]
```

2.  The game plan is to first do a quick pass through from data cleaning to analysis:
    i.  Take a toy dataset
    ii. "Clean" the data (minimally)
    iii. Run a regression model
3.  Once we have gotten comfortable with R, we will explore each topic in greater detail
    i.  [Data cleaning](#data_cleaning)
    ii. [Programming fundamentals](#prog_funda)
    iii. [Analysis: generalised linear models](#ana_glm)
    iv. [Presentation of results](#presentation)

## Resources

1.  [R courses](https://pll.harvard.edu/subject/r){target="_blank"}
2.  [The Epidemiologist R Handbook](https://epirhandbook.com/en/index.html){target="_blank"}
3.  [R for Data Science](https://r4ds.had.co.nz/index.html){target="_blank"}
4.  [Advanced R](https://adv-r.hadley.nz/){target="_blank"}
5.  [Cheat Sheets](https://posit.co/resources/cheatsheets/){target="_blank"}

# Getting comfortable with R

## What is an R package? {.smaller}

1.  It is a collection of code/data written by someone and packaged for distribution
2.  Hosted on the [*Comprehensive R Archive Network*](https://CRAN.R-project.org/package=palmerpenguins){target="_blank"} and/or [*Github*](https://github.com/allisonhorst/palmerpenguins/){target="_blank"} for public download
3.  From the CRAN webpage, we are able to find a reference manual documenting the data/functions of the package
4.  From the Github repository, we are able to see the actual codes implemented by the package's author(s)
5.  To install a package we would run `install.packages("palmerpenguins")` in the console
6.  To load a package, we would run `library(palmerpenguins)`
7.  To uninstall a package we would run `remove.packages("palmerpenguins")` in the console


## \[DEMO\] Intro to RStudio workspace {.smaller}

1.  Console: where you run/execute lines of code
1.  To assign a variable, we use the `<-` operator
    ```{r}
    x <- 1:10; print(x)
    ```
    i. random-access memory is allocated when a variable is assigned/declared
    i. random-access memory is freed-up when the variable is deleted (or garbage collected if it goes out of scope)
1.  Environment: where you're able to see variables stored in random-access memory
    i.  To clear variables from environment $\rightarrow$ execute `rm(list = ls())` in console

## \[DEMO\] Intro to RStudio workspace {.smaller}

1.  Script
    i. Writing code and saving the script
    i. Execute single/multiple lines of code $\rightarrow$ highlight code + Ctrl-enter
    i. Runs from top to bottom
    i. Comments will not be executed
1.  Hotkeys
    i. Clear the console $\rightarrow$ Ctrl-l
    i. Clear a line in console $\rightarrow$ Ctrl-d


## \[PRACTISE\] Tasks to do {.smaller}

1.  For this task, you will need the `palmerpenguins` and `tidyverse` packages. Install them if you have not already done so
2.  We will make use of the `penguins` dataset from `palmerpenguins` package
3.  Understand the data
4.  Dichotomise `body_mass_g` (continuous) into a categorical variable with 2 categories ("light": \<= 4750g and "heavy": \> 4750g)
5.  Run a linear regression[^1] with `body_mass_g` as the dependent variable and independent variables: `species`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm` and `sex`
6.  Run a logistic regression with the categorical body mass variable you created earlier as the dependent variable, with the same independent variables as before

[^1]: Note: the model is for learning purpose. No clever reason for choosing the variables




# Programming Fundamentals {#prog_funda}

## Data types {.smaller}

::: {.incremental}
1.  Character data type e.g. `"a"`
    ```{r}
    #| output-location: column-fragment
    typeof("a")
    ```
2.  Integer data type e.g. `1L`
    ```{r}
    #| output-location: column-fragment
    typeof(1L)
    ```
3.  Numeric data type e.g. `5.0`

    ```{r}
    #| output-location: column
    typeof(5.0)
    ```
4.  Logical data type e.g. `TRUE` or `FALSE`
    ```{r}
    #| output-location: column
    typeof(TRUE)
    ```
5.  Complex data type e.g. `1 + 4i`
    ```{r}
    #| output-location: column
    typeof(1 + 4i)
    ```
:::

## Data types (continued) {.smaller}
1.  Factor datatype for working with categorical variables
    ```{r}
    x <- factor(sample(c("Malay", "Others", "Indian", "Chinese"), 10, replace = T),
                levels = c("Chinese", "Malay", "Indian", "Others"))
    print(x)
    ```
    ```{r}
    print(levels(x))
    ```

1. A factor variable is more useful than a plain character vector. 
E.g. the first level will be used by `glm` as the reference category




## Data structures (vectors) {.smaller}

::: {.incremental}
1. Data structures are *containers* for holding data
1.  A character vector holds multiple characters
    ```{r}
    #| output-location: column
    letters
    ```
    ```{r}
    #| output-location: column
    typeof(letters)
    ```
1.  A numeric vector holds multiple numbers
    ```{r}
    #| output-location: column
    x <- 1:5; print(x)
    ```
    ```{r}
    #| output-location: column
    typeof(x)
    ```
:::

. . . 

::: callout-note
## Exercise
::: {.incremental}
1. Can you query the length of your character vector? `length(letters)`
1. Can you slice the character vector to get the first 5 elements? `letters[1:5]`
1. Can you slice the character vector to get the 1st, 9th, 20th elements? `letters[c(1, 9, 20)]`
1. Can you flip the character vector from last to first element? `letters[length(letters):1]`
:::
:::


## Data structures (lists) {.smaller}
1. Vectors can only hold 1 kind of data type e.g. characters, numeric etc
1. Lists are “containers” that can hold multiple data types
    1. Unnamed lists
       ```{r}
       list("a", 1, 10:15)
       ```
    1. Named lists
       ```{r}
       list(a = "a", b = 1, c = 10:15)
       ```

## Data structures (lists, continued)

::: callout-note
## Exercise

1. Create a variable in your console: `mylist <- list(a = "a", b = 1, c = 10:15)`
1. Can you query the length of mylist? `length(mylist)`
1. Can you get the second element of your mylist using
    1. index: `mylist[1]`
    1. key: `mylist$a` or `mylist[["a"]]`
1. Compare the difference between `mylist[3]` and `mylist[[3]]`
1. Compare the difference between `class(mylist[3])` and `class(mylist[[3]])`
:::

## Data structures (dataframe) {.smaller}

::: incremental
1. Dataframe is a *tabular container* that can hold multiple data types like lists, but each column can only store data of the same data type
1.  To view the first 2 rows of the `penguins` dataset from the `palmerpenguins` package

    ```{r}
    #| output-location: fragment
    #| code-line-numbers: "|3"
    library(tidyverse)
    library(palmerpenguins)
    penguins %>% head(2)
    ```

2.  To view the last 2 rows of the dataset

    ```{r}
    #| output-location: fragment
    penguins %>% tail(2)
    ```

3.  To view the data in rstudio, execute `view(penguins)`
:::

## Data structures (dataframe, continued)

::: callout-note

## Exercise
1. Can you query the dimensions of the `penguins` dataset using `dim(penguins)`, `ncol(penguins)`, `nrow(penguins)`?
1. Can you get a glimpse of the data using the functions `glimpse(penguins)`?
1. Can you view summary statistics of the data using `summary(penguins)`?
:::

## Data structures (dataframe, accessing variables) {.smaller}

1.  To get the column/variable-names of your dataset
    ```{r}
    colnames(penguins)
    ```
1.  To access any column variable, we use the `$` syntax
    ```{r}
    penguins$species[1:10]
    ```
1.  To get a table count of a variable or a cross-table count of 2 variables
    ```{r}
    table(penguins$sex, useNA = "ifany")
    ```
1. See [Data Cleaning](#data_cleaning) for more ways to work with dataframes


## Data structures (dataframe, loading data) {.smaller}

1. To import data from csv file, use `read_csv`
1. To import data from an excel file, use `read_excel`  
1. To import data from SAS/SPSS/Stata or export from R to these file formats,
use the [`haven`](https://haven.tidyverse.org/){target="_blank"} package


## Data structures (dataframe, adding labels) {.smaller}

1. To add labels to a dataframe, we make use of the [`labelled`](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html){target="_blank"} package



## Functions {.smaller}

1. A good chapter on explaining functions is [Chapter 6, Advanced R](https://adv-r.hadley.nz/functions.html){target="_blank"}, where most of the examples in the slides are taken
1.  Example function
    ```{r}
    f01 <- function(x, y) {
      # A comment
      x + y
    }
    ```
1. Components of a function
    a. *Arguments* $\rightarrow$ inputs to the function (there can also be functions with no arguments)
    a. *Body* $\rightarrow$ the code inside the function
    a. *Environment* $\rightarrow$ a list-like structure that stores/maps the variable name to value (a namespace as some people call it)
1. Functions are objects that can be assigned or they can be anonymous functions

## Function invocation {.smaller}
1.  How do we invoke the function?
    ```{r}
    print(f01)
    f01(3, 5)
    ```

1. How do we invoke one function after another?
    a. Suppose we have another function `f02`
       ```{r}
       f02 <- function(a, b) {
         a * 10 + b
       }
       ```
    b. To call `f01` first followed by `f02` and `sqrt` we will "nest" the functions (inside-out, right to left)
       ```{r}
       sqrt(f02(f01(1, 2), 5))
       ```

## Function invocation (continued) {.smaller}    

Another way to invoke a series of functions is to use the *pipe* operator `%>%` provided by the `magrittr` package
```{r}
library(magrittr)
value <- 1
value %>%
  f01(2) %>%
  f02(5) %>%
  sqrt()
```
    
::: callout-note
## Exercise
1. What is the difference between `f02` and `f02(1, 2)`?
:::

## Function output
1. What is returned by a function? The last evaluated expression

1. How do we store values outputted from a function? With assignment statment

1. Early termination of a function with a return statement

## Functions - argument passing {.smaller}

1. Pass by copy vs pass by reference
1. In R, all functions are pass by reference if you don't modify the object subsequently within the function.
i.e. copy-on-modify
1.  How to pass arguments?
    ```{r}
    f04 <- function(x, y, z) {
      (x / y) * z
    }
    ```
    a. By position e.g. `f04(10, 50, 20)`
    b. By name e.g. `f04(z = 20, y = 50, x = 10)`
    c. Unpacking multiple named arguments using `do.call`
       ```{r}
       do.call(f04, list(x = 10, y = 50, z = 20))
       ```

## Functions - lexical scoping, name masking 

Names defined inside a function mask names defined outside a function

```{r}
x <- 10
y <- 20
f05 <- function() {
  x <- 1
  y <- 2
  c(x, y)
}

f05()
```

## Functions - lexical scoping, looking one level up
R will look up a variable in the enclosing scope (e.g. within the function) and if it's not found,
will continue to proceed upwards (e.g. enclosing function or global environment) to look for
the variable until it is found 
```{r}
x <- 1
f06 <- function() {
  y <- 2
  i <- function() {
    z <- 3
    c(x, y, z)
  }
  i()
}

f06()
```

::: callout-note
## Exercise
Using the sample function, change the z variable inside the function and observe that...
```{r}
z <- 10
f07 <- function(x, y) {
  # make your edit here
  z <- z * 100
  z + x + y
}
```
...there is no change to the z variable outside the function in the global environment
:::

## Functions - lexical scoping, dynamic lookup 

R looks for the values when the function is run, not when the function is created
```{r}
z <- 10
f08 <- function(x, y) {
  z + x + y
}

z <- 100
f08(5, 10)
```

## Functions

## Loops

## Debugging

## Memory Management


# Data Cleaning {#data_cleaning}

## Common functions - packages

1.  These functions are from various packages, conveniently collected in the [tidyverse](https://www.tidyverse.org/){target="_blank"} package
2.  The best package for data wrangling is [dplyr](https://dplyr.tidyverse.org/){target="_blank"}. Useful info can be found in this [chapter](https://r4ds.had.co.nz/transform.html){target="_blank"}
3.  A package for working with dates is [lubridate](https://lubridate.tidyverse.org/){target="_blank"}
4.  A package for working with strings is [stringr](https://stringr.tidyverse.org/){target="_blank"}. This is useful for cleaning free-text response data
5.  A package for working with factor datatype is [forcats](https://forcats.tidyverse.org/){target="_blank"}


## Common functions - dplyr::select {.smaller}

The [`select`](https://dplyr.tidyverse.org/reference/select.html){target="_blank"} function enables you to select a subset of the columns in your dataset

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|4"
# library(tidyverse) # if not already loaded
# library(penguins) # if not already loaded
penguins %>%
  select(species, island, bill_length_mm) %>%
  head(3)
```

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|3"
# there is also ends_with
penguins %>%
  select(starts_with("bill")) %>%
  head(3)
```

. . .

<br>

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|2"
penguins %>%
  select(matches("mm")) %>%
  head(3)
```

## Common functions - dplyr::filter {.smaller}

The [`filter`](https://dplyr.tidyverse.org/reference/filter.html){target="_blank"} function enables you to select a subset of the rows that meet a certain criteria

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  filter(body_mass_g > 3500) %>%
  head(3)
```

. . .

```{r}
#| output-location: fragment
penguins %>% filter(sex == "female")
```

## Common functions - dplyr::mutate {.smaller}

The [`mutate`](https://dplyr.tidyverse.org/reference/mutate.html){target="_blank"} function enables you to add columns to your dataset. The added columns can be derived from existing column(s)

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  mutate(body_mass_100g = body_mass_g / 100) %>%
  select(body_mass_g, body_mass_100g) %>%
  head(5)
```

. . .

```{r}
#| output-location: fragment
#| code-line-numbers: "|2"
penguins %>%
  mutate(bill_length_plus_depth_mm = bill_length_mm + bill_depth_mm) %>%
  select(matches("bill")) %>%
  head(5)
```

## Common functions - dplyr::group_by, summarise {.smaller}

::: incremental
1.  The [`summarise`](https://dplyr.tidyverse.org/reference/summarise.html){target="_blank"} function enables you to get summary statistics like N, mean, median etc

    ```{r}
    #| output-location: fragment
    penguins %>% summarise(N = n(),
                           mean_body_mass_g = mean(body_mass_g))
    ```

2.  The [`group_by`](https://dplyr.tidyverse.org/reference/group_by.html){target="_blank"} function enables you to get summary statistics within groups

    ```{r}
    #| output-location: fragment
    #| code-line-numbers: "|2"
    penguins %>%
      group_by(sex) %>%
      summarise(N = n(),
                mean_body_mass_g = mean(body_mass_g),
                median_body_mass_g = median(body_mass_g))
    ```
:::

## Common functions - dplyr::arrange

The function [`arrange`](https://dplyr.tidyverse.org/reference/arrange.html){target="_blank"} enables us to sort by a certain variable

```{r}
#| output-location: fragment
#| code-line-numbers: "|4"
penguins %>%
  group_by(sex, year) %>%
  summarise(mean_body_mass_g = mean(body_mass_g)) %>%
  arrange(desc(year))
```

## Common functions - dplyr::if_else, case_when {.smaller}

The functions [`if_else`](https://dplyr.tidyverse.org/reference/if_else.html#arguments){target="_blank"} and [`case_when`](https://dplyr.tidyverse.org/reference/case_when.html){target="_blank"} are often used with mutate to create new variables

. . .

```{r}
#| output-location: fragment
#| code-line-number: "|3"
set.seed(2)
penguins %>%
  mutate(bill_length_type = if_else(bill_length_mm >= 48.5, "long bill", "short bill")) %>%
  sample_n(4)
```

. . .

```{r}
#| output-location: fragment
penguins %>%
  mutate(bill_length_type = case_when(bill_length_mm >= 48.5 ~ "long bill",
                                      bill_length_mm < 48.5 ~ "short bill",
                                      TRUE ~ NA_character_)) %>%
  select(bill_length_type) %>%
  table(useNA = "ifany")
```

## Common functions - forcats::fct_relevel {.smaller .scrollable}

1. If we convert a character vector to a factor vector, the levels will be arranged in alphabetical order
   automatically if unspecified
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>% factor()
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. We can specify the levels of the factor using the `levels` argument
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         factor(levels = c("short bill", "medium bill", "long bill"))
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. The factor vector will only contain values specified by `levels`.
   Any existing character values unspecified in `levels` (e.g. "long bill") will be converted to NA
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         factor(levels = c("short bill", "medium bill"))
     ) %>%
     .$bill_length_type %>% levels()
   ```

1. [fct_relevel](https://forcats.tidyverse.org/reference/fct_relevel.html){target="_blank"} relevels levels of a factor vector (if not already a factor eg. character vector, it will first convert it to a factor vector) Any unspecified levels will follow after specified ones, left in the existing order
   ```{r}
   penguins %>%
     mutate(bill_length_type = case_when(
       bill_length_mm < 40 ~ "short bill",
       between(bill_length_mm, 40, 50) ~ "medium bill",
       bill_length_mm > 50 ~ "long bill",
       TRUE ~ NA_character_) %>%
         fct_relevel("short bill", "medium bill")
     ) %>%
     .$bill_length_type %>% levels()
   ```

## Common functions - forcats::fct_collapse {.smaller .scrollable}

1. We can collapse levels of a factor variable using forcat's [fct_collapse](https://forcats.tidyverse.org/reference/fct_collapse.html){target="_blank"} function
1. The `island` variable has 3 levels
   ```{r}
   penguins$island %>% table()
   ```
1. Suppose we want to collapse `island` into 2 levels, we can apply `fct_collapse` like so
   ```{r}
   penguins %>%
     mutate(island = fct_collapse(island, "Dream/Torgersen" = c("Dream", "Torgersen"))) %>%
     .$island %>%
     table()
   ```

## Common functions - tidyr::pivot_wider {.smaller .scrollable}

1. Suppose we have some data in long format

   ```{r}
   # Generate fake patient data for illustration
   generate_fake_patient_data <- function(id) {
     data.frame(list(
       patient_id = id,
       time = c("baseline", "12mth"),
       hba1c = runif(n = 2, min = 4, max = 14),
       ldl = sample(c("good control", "bad control"))
     ))
   }
   
   set.seed(2024)
   data <- map(
     sprintf("P%s", str_pad(1:344, width = 2, pad = 0)),
     generate_fake_patient_data
   ) %>%
     bind_rows()
   print(head(data, 10))
   ```

1. We can make use of tidyr's [pivot_wider](https://tidyr.tidyverse.org/reference/pivot_wider.html){target="_blank"} to reshape our data
   from long to wide
   ```{r}
   data_wide <- data %>%
     pivot_wider(id_cols = patient_id,
                 names_from = "time",
                 names_glue = "{.value}_{time}",
                 values_from = c(hba1c, ldl)) 
   head(data_wide, 15)
   ```

## Common functions - tidyr::pivot_longer {.smaller .scrollable}

1. Suppose instead that we were given data in a wide format
   ```{r}
   print(head(data_wide, 10))
   ```

1. We can make use of tidyr's [pivot_longer](https://tidyr.tidyverse.org/reference/pivot_longer.html){target="_blank"}
   to reshape our data into long format
   ```{r}
   data_wide %>%
     pivot_longer(
       cols = matches("baseline|12mth"),
       names_pattern = "(.*)_(baseline|12mth)",
       names_to = c(".value", "time")
     )
   ```


## Common functions - dplyr::bind_rows, bind_cols 

## Common functions - dplyr::mutate + across {.smaller .scrollable}

1. This combination of `mutate` & `across` enables us to apply a function to multiple columns of a dataframe
   ```{r}
   penguins %>%
     mutate(across(.cols = c(bill_length_mm, bill_depth_mm), 
                   .fns = ~ floor(.x),
                   .names = "{.col}_floored")) %>%
     select(matches("bill_(length|depth)"))
   ```

1. If we omit the `.names` argument, the function will be applied in-place
   ```{r}
   penguins %>%
     mutate(across(.cols = c(bill_length_mm, bill_depth_mm), 
                   .fns = ~ floor(.x))) %>%
     select(matches("bill_(length|depth)"))
   ```

## Common functions - stringr::str_detect {.smaller .scrollable}

1. We can make use of [regular expressions](https://rstudio.github.io/cheatsheets/strings.pdf){target="_blank"} in
   stringr's [str_detect](https://stringr.tidyverse.org/reference/str_detect.html){target="_blank"} function together with 
   `filter`, or `if_else`/`case_when` for data cleaning

1. Here we create a `species_plus` variable
   ```{r}
   set.seed(2024)
   penguins <- penguins %>%
     mutate(across(
       .cols = "species",
       .fns = ~ paste(.x,
         sample(c("baby", "teenager", "adult"), nrow(penguins), replace = TRUE),
         sex,
         sep = "_"
       ),
       .names = "{.col}_plus"
     ))
   # view counts of species_plus
   table(penguins$species_plus)
   ```

1. We can use a regular expression to identify categories
   ```{r}
   penguins %>%
     mutate(across(
       .cols = "species_plus",
       .fns = ~ case_when(
         # Chinstrap/Gentoo, not followed by baby, male/female
         str_detect(.x, "((Chinstrap|Gentoo)(?!_baby)).*(male|female)") ~ "DO SOMETHING",
         TRUE ~ .x # maintain existing value
       ),
       .names = "{.col}2" 
     )) %>%
     .$species_plus2 %>%
     table()
   ```




# Data visualisation

The [ggplot2](https://ggplot2.tidyverse.org/){target="_blank"} package is well known for plotting figures

## Histograms

```{r}
penguins %>%
  ggplot(aes(x = flipper_length_mm, fill = species)) + geom_histogram(bins = 40)
```

## Facet wrap

```{r}
penguins %>%
  ggplot(aes(x = flipper_length_mm, fill = sex)) +
    geom_histogram() +
    facet_wrap(~species, dir = "v", scale = "free_y")
```




# Analysis: Descriptive statistics

## Analysis: Bivariate tests for independent observations {.smaller .scrollable}

| Variable 1  | Variable 2  | Bivariate test                                                                                          |
| ----------- | ----------- | ------------------------------------------------------------------------------------------------------- |
| Categorical | Categorical | 1. Chi-square test <br> 2. Fisher's exact test                                                          |
| Categorical | Continuous  | Parametric: <br> 1. Independent t-test (2 categories) <br> 2. One-way independent ANOVA (>2 categories) |
| Categorical | Continuous  | Non-parametric: <br> 1. Mann-Whitney test (2 categories) a.k.a. Wilcoxon rank-sum test <br> 2. Kruskal-Wallis test (>2 categories) |
| Continuous  | Continuous  | Parametric: Pearson's correlation coefficient                                                           |
| Continuous  | Continuous  | Non-parametric: Spearman's correlation coefficient                                                      |

: {tbl-colwidths="[25,25,50]"}

## Analysis: Bivariate tests for repeated measurements {.smaller .scrollable}

| Variable 1  | Variable 2  | Bivariate test                                                               |
| ----------- | ----------- | ---------------------------------------------------------------------------- |
| Categorical | Categorical time <br> (e.g. baseline, 12-month) | McNemar's test                            |
| Continuous  | Categorical time <br> (e.g. baseline, 12-month) | Parametric: Dependent t-test              |
| Continuous  | Categorical time <br> (e.g. baseline, 12-month) | Non-parametric: Wilcoxon signed-rank test |

: {tbl-colwidths="[25,37.5,37.5]"}


## Measures of central tendency {.smaller .scrollable}

1. Mean (SD), median (IQR) helps you get a sense of the distribution of characteristics in your study population with respect to levels of the outcome
1. The [`tableby` function](https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html){target="_blank"} of the [`arsenal`](https://mayoverse.github.io/arsenal/){target="_blank"} package lets you do this easily
1. We will talk about improving upon the formatting of the table in [presenting tables](#pres_table)

```{r}
#| code-line-numbers: "|3,4,5"
library(arsenal)

tableby(species ~ sex + island + bill_length_mm,                                                        # <1>
        data = penguins,                                                                                # <1>
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"))) %>%     # <1>
summary(text = TRUE) %>%
knitr::kable(align = c("l", rep("r", 5)))
```

1. `tableby` function invoked by these 3 lines


## Measures of central tendency (repeated measures) {.scrollable}

::: {.r-fit-text}

1. Suppose we have repeated measures of HbA1c and LDL from patients at e.g. baseline and 12-month
   ```{r}
   generate_fake_patient_data <- function(id) {
     data.frame(list(
       patient_id = id,
       time = c("baseline", "12-month"),
       hba1c = runif(n = 2, min = 4, max = 14),
       ldl = sample(c("good control", "bad control"))
     ))
   }
   
   set.seed(2024)
   data <- map(
     sprintf("P%s", str_pad(1:30, width = 2, pad = 0)),
     generate_fake_patient_data
   ) %>%
     bind_rows() %>%
     mutate(across(.cols = time,
                   .fns = ~ fct_relevel(.x, "baseline", "12-month")))
   print(head(data, 10))
   ```

1. The [`paired` function](https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html){target="_blank"} from [`arsenal`](https://mayoverse.github.io/arsenal/){target="_blank"} package helps us tabulate pre-post tests easily

   ```{r}
   # pacman::p_load(arsenal, kableExtra)
   
   table <- paired(
     time ~ signed.rank(hba1c) + ldl,
     data = data,
     id = patient_id,
     control = paired.control(digits = 2,
                              numeric.stats = c("Nmiss2", "meansd", "medianq1q3", "range"),
                              numeric.test = "signed.rank",
                              signed.rank.correct = FALSE))

   table %>%
     summary(text = TRUE) %>%
     kable(align = c("l", rep("r", 4))) %>%
     column_spec(column = 1, width_min = "4cm") %>%
     column_spec(column = 2:4, width_min = "3cm") %>%
     column_spec(column = 5, width_min = "2.5cm")
   ```

   ```{r}
   tests(table) %>%
   kable()
   ```

:::




# Analysis: Generalised Linear Models (GLM) {#ana_glm}

## How are categorical variables represented? {.smaller}

1. Categorical variables are characters, but we need a numeric matrix in order to perform computations to get the $\beta$ coefficients
```{r}
set.seed(10)
idx <- sample(1:344, 5)
penguins[idx,]
```

1. So one-hot encoding / dummy encoding will be applied to categorical variables
```{r}
model.matrix(~species, penguins)[idx,]
```


## Framework {.smaller}

```{mermaid}
%%| echo: false
%%| fig-width: 9

flowchart LR
  A[Model specification] --> B[Inference] --> C[Diagnostics]
```

1.  Model specification
    a.  How to decide what type of model to fit?
    b.  How to decide what variables to choose?
2.  Inference
    a.  How do we get the value of the coefficients, confidence intervals, p-values?
3.  Diagnostics
    a.  Does the model fit our data well?

<!-- ```{=html} -->
<!-- ## Model specification

1.  Look at the literature
2.  Form an idea of a causal diagram
3.  Check for multicollinearity -->
<!-- ``` -->


## Why the need for GLM? {.smaller}

1.  We want to find the association between an outcome (e.g. BMI) and some dependent variables (e.g. age, gender, ethnicity, social economic status)
2.  We are used to fitting a multivariable linear regression $$
    \begin{align}
      \underbrace{BMI}_{\text{continuous}} &= \underbrace{\beta_0 + \beta_1age + \beta_2gender + \beta_3ethnicity + \cdots + \epsilon}_{\text{this linear combination is continuous}} \\
      \epsilon &\sim N(0, \sigma^2)
    \end{align}
    $$
3.  What if your outcome is not continuous?
    i.  E.g. outcome takes on values of 0 or 1 (logistic regression)
    ii. E.g. outocme takes on discrete integer values 0, 1, 2, ... (poisson regression)
4.  GLM gives you a way to relate the outome (which can take on various distributions) with a linear combination of dependent variables

## Formal definition of GLM {.smaller .scrollable}

GLM consists of 2 components

::: {.callout-tip}
# Component 1: What **distribution** does your outcome, $y_i$ take on?

\begin{align}
    y_i|X_i \sim \text{member of exponential family} \ \text{(e.g. $y_i$ is normally distributed)}
\end{align}

i. $i$ indexes the $i$-th observation of your dataset i.e. corresponds to a particular row of your dataset
i. $y_i$ represents the $i$-th observation of your outcome variable
i. $X_i$ represents the $i$-th observation of all your independent variables (e.g. age, gender, ethnicity)

:::

::: {.callout-warning}
# Component 2: What is the **link function** $g$ you want to use?

\begin{align}
    g(\mathbb{E}(y_i|X_i)) = X_i^{T} \beta = \underbrace{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}_{\text{this linear combination is continuous}}
\end{align}

i.  $g$ is the link function that maps $\mathbb{E}(y_i|x_i)$ to $X_i^{T}\beta$
i.  It is the link function that enables the mapping of the continuous $X_i^{T}\beta$ to $y_i$, which can be discrete for example
:::

2.  In a generalised linear model, the outcome variable $y_1,...,y_n$ are modelled as independent and identically distributed (iid) observations

2.  The outcome variable is treated as a random variable (i.e. outcome takes on a certain distribution e.g. normal), but NOT the independent variables

## Family member 1: Linear regression {.smaller}

When our outcome, $y_i$ is continuous and takes on real values (i.e. $y_i \in \mathbb{R}$), we may choose to model $y_i$ with a normal distribution

::: {.callout-tip}
## Component 1: $y_i|x_i$ is normally distributed
$$
y_i | x_i \sim \mathcal{N}(X_i^{T}\beta, \sigma^2)
$$
:::

::: {.callout-warning}
## Component 2: link function is the identity function i.e. no transformation done
$$
\mathbb{E}(y_i|x_i) = X_i^{T}\beta = \beta_0 + \beta_1age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots
$$
:::

------------------------------------------------------------------------

## Running a linear regression in R {.smaller}

1.  Notice the use of the formula syntax. LHS of the `~` is the dependent variable, while RHS are the independent variables
2.  Notice the `family` argument contains the 2 components of the GLM we discussed earlier

```{r}
#| output-location: column-fragment
#| code-line-numbers: "|2|6|8"
model <- 
  glm(body_mass_g ~ species + bill_length_mm +
                    bill_depth_mm + 
                    flipper_length_mm + sex,
      data = penguins,
      family = gaussian(link = "identity"))

summary(model)
```

------------------------------------------------------------------------

## Family member 2: Logistic regression {.smaller .scrollable}

When our outcome, $y_i$, is discrete and dichotomous (i.e. take two discrete values, 0-1, success-failure), we can model $y_i$ with a Bernoulli distribution

::: {.callout-tip}
## Component 1: $y_i|x_i$ is Bernoulli distributed

$$
y_i | x_i \sim \mathrm{Bernoulli}(\pi_i)
$$

- $\pi_i$ is the probability of success of $y_i$
- $\mathbb{E}(y_i|x_i) = \pi_i$
:::

::: {.callout-warning}
## Component 2: Link function, $g$, is the logit function
\begin{align}
    g(\mathbb{E}(y_i|x_i)) &= \underbrace{\mathrm{ln} \left( \frac{\mathbb{E}(y_i|x_i)}{1-\mathbb{E}(y_i|x_i)} \right)}_{logit[\mathbb{E}(y_i|x_i)]} = X_i^{T}\beta = \underbrace{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}_{\text{this linear combination is continuous}} 
\end{align}
:::

------------------------------------------------------------------------

## Running a logistic regression in R

1.  To be filled only after completion of exercise
2.  Code is largely similar to linear regression except for:
    i.  `family = binomial(link = "logit")`

## Inferring the coefficients {.scrollable}

::: {.r-fit-text}
1.  *Maximum likelihood estimation (MLE)* is the way to infer the beta coefficients
1. The idea behind MLE is that we want to maximise the joint probability of our observed data assuming that the data was generated according to our model i.e. we want to maximise the likelihood function $\mathcal{L}(\beta) = P(y_1, y_2, ..., y_n)$.
1. In the case of logistic regression, the likelihood function $\mathcal{L}(\beta)$ is:
    \begin{align}
        \mathcal{L}(\beta) &= P(y_1, y_2, ..., y_n) \\
        &= \prod_{i=1}^{n} P(y_i) \\
        &= \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
    \end{align}
1. Convert the likelihood to log-likehood to make it easier to work with (we can do that because log is a monotonic function)
    \begin{align}
        \mathrm{ln}\ \mathcal{L}(\beta)
        %
        &= \mathrm{ln} \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
        %
        &= \sum_{i=1}^{n} \left[ y_i\mathrm{ln}\pi_i + (1-y_i)\mathrm{ln}(1-\pi_i) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \mathrm{ln} \left( \frac{e^{\beta^{T} x_i}}{1+e^{\beta^{T} x_i}} \right) + (1-y_i) \mathrm{ln} \left( 1-\frac{e^{\beta^{T} x_i}}{1+e^{\beta^{T} x_i}} \right) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - y_i \mathrm{ln}(1+e^{\beta^{T}x_i}) + (1-y_i) \mathrm{ln} \left( \frac{1}{1+e^{\beta^{T}x_i}} \right) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - y_i \mathrm{ln}(1+e^{\beta^{T}x_i}) - (1-y_i) \mathrm{ln} (1 + e^{\beta^{T}x_i} ) \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i \beta^{T}x_i - \mathrm{ln}(1+e^{\beta^{T}x_i}) \right] \\
    \end{align}
1. In order to maximise the log-likelihood, we will use the iteratively reweighted least squares (IRLS) algorithm which is a Newton-Raphson like method
    \begin{align}
    \beta^{(t+1)} = \beta^{(t)} - \textit{H}^{-1}(\beta^{(t)}) \nabla_{\beta} \mathcal{L}(\beta^{(t)})
    \end{align}
    The idea behind this is to update $\beta$ via a series of iterations such that on every update, we are stepping in the direction of steepest ascent.
1. To use the IRLS, first we need to calculate the gradient of the log-likelihood
    \begin{align}
        \nabla_{\beta} \mathrm{ln}\ \mathcal{L}(\beta)
        &= \sum_{i=1}^{n} \left[ y_i \nabla_\beta \beta^{T}x_i - \nabla_\beta \mathrm{ln}(1+e^{\beta^{T}x_i})  \right] \\
        %
        &= \sum_{i=1}^{n} \left[ y_i x_i - \frac{e^{\beta^{T}x_i}}{1+e^{\beta^{T}x_i}} x_i \right] \\
        %
        &= \sum_{i=1}^{n} (y_i - \pi_i) x_i \\
        & = X^{T}(y - \pi)
    \end{align}
1. Next, we will calculate the Hessian, $H$, of the log-likelihood by taking derivatives one more time from the gradient.
    \begin{align}
    H &= -\sum_{i=1}^{n} \pi_i (1-\pi_i) x_i x_i^{T} \\
    &= - X^{T} W X \\
    \end{align}

    where $W$ is a diagonal weight matrix given by:
    \begin{align}
        W = diag\{\pi_1 (1-\pi_1), \pi_2 (1-\pi_2), ..., \pi_n (1-\pi_n)\}
    \end{align}
1. Now that we have the components, we can substitute these back into (5) to expand it out
    \begin{align}
        \beta^{(t+1)} = \beta^{(t)} + (X^{T}W^{(t)}X)^{-1}X^{T}(y - \pi^{(t)})
    \end{align}
:::

## Interpret coefficients: logistic regression {.scrollable}

::: {.r-fit-text}
1. Step 1: Start from the equation linking outcome to indepenent variables
    \begin{align}
    \mathrm{ln}\left(\underbrace{\frac{\mathbb{E}(y_i|x_i)}{1-\mathbb{E}(y_i|x_i)}}_{odds_i}\right) = \beta_0 &+ \beta_1 age_i + \beta_2 gender^{(male)}_i \\ 

    &+ \beta_3 ethnicity^{(malay)}_i + \beta_4 ethnicity^{(indian)}_i + \cdots
    \end{align}

1. Step 2: Analyse unit changes

    i. Continuous example: 1 unit increase in age, with everything else held constant. The exponent of $\beta_1$ gives us the odds ratio for every unit increase in age.
        \begin{align}
            \mathrm{ln}\ odds_i|_{age=a+1} - \mathrm{ln}\ odds_i|_{age=a} &= \beta_1 (a+1) - \beta_1 (a) \\
            \mathrm{ln}\ \frac{odds_i|_{age=a+1}}{odds_i|_{age=a}} &= \beta_1 \\
            \frac{odds_i|_{age=a+1}}{odds_i|_{age=a}} &= e^{\beta_1}
        \end{align}
    
    i. Categorical example: compare to reference category, with everything else held constant. The exponent of $\beta_3$ gives us the odds ratio of malay compared to chinese.
        \begin{align}
        \mathrm{ln}\ odds_i|_{ethnicity=malay} - \mathrm{ln}\ odds_i|_{ethnicity=chinese} &= \beta_3 \\
        \mathrm{ln}\ \frac{odds_i|_{ethnicity=malay}}{odds_i|_{ethnicity=chinese}} &= \beta_3 \\
        \frac{odds_i|_{ethnicity=malay}}{odds_i|_{ethnicity=chinese}} &= e^{\beta_3}
        \end{align}

::: {.aside}
Notes: 

1. $odds_i$ represents the odds of "success" of the outcome, which is coded as 1

1. Reference categories do not have $\beta$ coefficient due to one-hot encoding e.g. gender: female, ethnicity: chinese
:::

:::


## Family member 3: Gamma regression {.smaller .scrollable} 

When our outcome $y_i$, is continuous and positive, we can model $y_i$ with a Gamma distribution

::: {.callout-tip}
## Component 1: $y_i$ is Gamma distributed
$$ 
y_i | X_i \sim Gamma(\alpha, \beta)
$$
:::

::: {.callout-warning}
## Component 2: One choice of the link function, $g$, is the ln function
$$
\begin{align*}
    g(\mathbb{E}(y_i|X_i)) = \mathrm{ln}\ \mathbb{E}(y_i|X_i) = \beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots \\
\end{align*} 
$$
:::

Notes:

1. The inverse of the ln function is the exponent function. Alternatively, we can also express $\mathbb{E}(y_i|x_i) = e^{\beta_0 + \beta_1 age_i + \beta_2 gender_i + \beta_3 ethnicity_i + \cdots}$

1. From this expression, we can see that the inverse of the link function (i.e. exponent) maps $X_i^{T}\beta$ to $\mathbb{E}(y_i|X_i)$. And we know that this is a valid function because $\mathbb{E}(y_i|X_i)=e^{X_i^{T}\beta} > 0$


## Interpret coefficients: gamma regression {.scrollable}

::: {.r-fit-text}
1. Step 1: Start from the equation linking outcome to independent variables
    \begin{align}
    \mathrm{ln}\ \mathbb{E}(y_i|X_i) = \beta_0 &+ \beta_1 age_i + \beta_2 gender^{(male)}_i \\
    &+\beta_3 ethnicity^{(malay)}_i + \beta_4 ethnicity^{(indian)}_i + \cdots \\
    %
    \end{align}
1. Step 2: Analyse unit changes
    i. Continuous example: 1 unit increase in age, with everything else held constant. The percentage change in mean outcome for every unit increase in age can be given from $e^{\beta_1} - 1$
        \begin{align}
            \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a+1} - \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a} &= \beta_1 (a+1) - \beta_1(a) \\
            %
            \mathrm{ln}\ \frac{\mathbb{E}(y_i|X_i)|_{age=a+1}}{ \mathbb{E}(y_i|X_i)|_{age=a}} &= \beta_1 \\
            %
            \frac{\mathbb{E}(y_i|X_i)|_{age=a+1}}{ \mathbb{E}(y_i|X_i)|_{age=a}} -1 &= e^{\beta_1} - 1
        \end{align}
    i. Interaction terms example: compare vs reference category, age at some value $a$, with everything else held constant
        a. The percentage change in mean outcome for male compared to female can be given from $e^{\beta_2 + \beta_3 a} - 1$ 
        a. Notice now that this percentage change which was $\beta_2$ (if no interaction had been defined) is now modified by $\beta_3 a$ which depends on age
        \begin{align}
            \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a, gender=male} - \mathrm{ln}\ \mathbb{E}(y_i|X_i)|_{age=a, gender=female} &= (\beta_1 a + \beta_2 + \beta_3 a) - \beta_1 a  \\
            %
            \mathrm{ln}\ \frac{\mathbb{E}(y_i|X_i)|_{age=a, gender=male}}{ \mathbb{E}(y_i|X_i)|_{age=a, gender=female}} &= \beta_2 + \beta_3 a \\
            %
            \frac{\mathbb{E}(y_i|X_i)|_{age=a, gender=male}}{ \mathbb{E}(y_i|X_i)|_{age=a, gender=female}} -1 &= e^{\beta_2 + \beta_3 a} - 1
        \end{align}
:::

## Family member 4: Conway-Maxell-Poisson (CMP) regression {.smaller}

1. When our outcome variable is a "count", the poisson regression model is used because the poisson distribution models counts better than say
linear regression which assumes that the outcome variable is normally distributed
1. However, the poisson regression model is restrictive as it assumes equi-dispersion in our outcome variable (i.e. mean of outcome variable approximately equals the variance of the outcome variable)
When there is under- or over-dispersion, the poisson distribution no longer models the outcome variable well.
1. When there is over-dispersion (variance of outcome > mean of outcome), negative binomial regression is often used
1. When there is under-dispersion (i.e. variance of outcome < mean of outcome), we will use the Conway-Maxwell-Poisson regression model
1. Useful resources: (i) Video explanations see [@videolectureschannelFlexibleModelCount2012;@consortiumObservationDrivenConwayMaxwell2018] (ii) Sellers & Premeaux [@sellersConwayMaxwellPoisson2020] explains the CMP regression and surveys available statistical packages

## CMP distribution {.smaller}

<!-- 1. Background info on the Conway-Maxwell-Poisson (CMP) distribution: -->
1. The Conway-Maxwell-Poisson distribution differs from the Poisson distribution with the introduction of a parameter $\nu$
1. The $\nu$ parameter enables the Conway-Maxwell-Poisson distribution to model a wide range of dispersion, and generalises well-known distributions
    a. When $\nu = 1$, it takes on the Poisson distribution (equi-dispersion)
    a. When $\nu = 0$ and $\lambda < 1$, it takes on the geometric distribution (over-dispersion)
    a. When $\nu \rightarrow \infty$, it takes on the bernoulli distribution (under-dispersion)

## CMP regression {.smaller}
1. Use of the Conway-Maxwell-Poisson (CMP) distribution in the generalised linear model (GLM) setting:
    i. In the original CMP formulation of Sellers and Shmueli[@sellersFlexibleRegressionModel2010], the relationship between
       dependent variable $Y$ and independent variables $X$ is given by the equations:
        a. $ln(\lambda_{i}) = X_{i}^T\beta$
        a. $\mathbb{E}(Y_{i}) = \lambda_{i} \frac{\partial ln Z(lambda_{i}, \nu)}{\partial \lambda_i} \approx \lambda_{i}^{\frac{1}{\nu}} - \frac{\nu - 1}{2\nu}$,
           where $Z(\lambda_i, \nu_i)$ is a normalizing constant
1. However, the CMP regression model is not amenable to interpretation of coefficients by mean contrasts, because $\lambda_i$ is related to $\mathbb{E}(Y_i)$ by a non-linear function
1. To overcome this, Huang 2017[@huangMeanparametrizedConwayMaxwell2017] reparameterised the CMP distribution to model the mean directly. He called it CMP~$\mu$~

## CMP~$\mu$~-regression {.smaller}

1. Under the CMP~$\mu$~-regression formulation, the relationship between dependent variable $Y$ and independent variables $X$ is given by $ln(\mathbb{E}(Y_i|X)) = X_{i}^T\beta$
1. The convenient thing about CMP-regression and CMP~$\mu$~-regression is that we can conduct a likelihood ratio test[@sellersFlexibleRegressionModel2010;@huangMeanparametrizedConwayMaxwell2017]
   to see if a poisson regression model (poisson regression is a special case of CMP-regression when $\nu = 1$) is adequate (i.e. $H_0: \nu = 1$ vs $H_1: \nu \ne 1$) 
1. Note that $H_1$ does not specify the direction (under vs over) of data dispersion. This can be assessed via the dispersion estimate $\hat{\nu}$[@sellersFlexibleRegressionModel2010]
   (i.e. $\hat{\nu} > 1$ if under-dispersion and $\hat{\nu} < 1$ if over-dispersion)
1. Model diagnostics[@huangMeanparametrizedConwayMaxwell2017]: If the underlying distributional model is correct, the probability inverse transformation (PIT) should resemble
   a random sample from a standard uniform distribution. Goodness-of-fit can then be assessed by plotting a histogram of the PIT, or via a quantile plot of the PIT against
   the uniform distribution
    
## Running a CMP~$\mu$~-regression in R

We will use the [mpcmp](https://thomas-fung.github.io/mpcmp/){target="_blank"}[@fungMpcmpMeanParametriziedConwayMaxwell2020] R package's implementation of the CMP~$\mu$~-regression




# Analysis: Concordance

## What is the Kappa statistic?

1.  The Kappa statistic is a measure of "true" agreement. It indicates the proportion of agreement beyond that expected by chance [@simKappaStatisticReliability2005]
2.  If prevalence index is high, chance agreement will be higher, kappa is reduced
3.  If bias index is high, chance agreement will be lower, kappa is higher

## Calculation of Kappa statistic

1.  Calculation of prevalence-adjusted, bias-adjusted Kappa can be done through the [`epiR`](https://cran.r-project.org/web/packages/epiR/index.html){target="_blank"} package

<!-- ## Sample size calculation -->




# Analysis: Tests of reliability

## Internal Consistency

::: {.r-fit-text}

1. Cronbach's alpha is a measure of internal consistency of questionnaire
1. Interpreting Cronbach’s alpha
    i. Cronbach’s alpha between 0.70 to 0.80 is considered good enough[@fieldDiscoveringStatisticsUsing2012a]
    i. Cronbach’s alpha is shown as raw_alpha in the output
1. Caveats:
    i. All else equal, alpha will increase as the number of items on the scale increases[@fieldDiscoveringStatisticsUsing2012a] (page 799, section 17.8.2)
    i. If responses lack variability (i.e. almost everyone gives the same response), alpha tends to be poor[@fieldDiscoveringStatisticsUsing2012a] (page 803)
    i. "If alpha is too high it may suggest that some items are redundant as they are testing the same question but in a different guise. A maximum alpha value of 0.90 has been recommended" [@tavakolMakingSenseCronbach2011a]

:::

## Test-retest reliability

::: {.r-fit-text}

1. We will use the Intraclass Correlation Coefficient to assess for test-retest reliability
1. Out of the 10 variants of ICC, we will compute the "two-way mixed effects, absolute agreement, single rater/measurement" ICC variant or ICC (A,1) of McGraw and Wong[@mcgrawFormingInferencesIntraclass1996] per recommendations from[@kooGuidelineSelectingReporting2016a; @qinAssessingTestretestReliability2019]:
1. When it comes to interpreting the ICC, according to Koo et al.[@kooGuidelineSelectingReporting2016a]:
    i. ICC less than 0.5: poor reliability
    i. ICC between 0.5 and 0.75: moderate reliability
    i. ICC between 0.75 and 0.90: good reliability
    i. ICC greater than 0.90: excellent reliability

:::




# Analysis: Comparative effectiveness research using observational data

## What is comparative effectiveness research? {.smaller}

1. Comparative effectiveness research is the generation and synthesis of evidence that compares the benefits and harms of alternative methods to prevent, diagnose, treat, and monitor a clinical condition or to improve the delivery of care [@gatsonisMethodsComparativeEffectiveness2015]
1. _Effectiveness_ vs _efficacy_
    i. _Effectiveness_ refers to the performance of an intervention in "real-world" clinical settings while _efficacy_ refers to the performance of an intervention under "ideal" circumstances [@gatsonisMethodsComparativeEffectiveness2015]
1. Examples:
    i. Compare the effectiveness of different treatment options [@jaksaUsingPrimaryCare2022]
    i. Compare the effectiveness of a health intervention programme vs usual care group [@luImpactLongitudinalVirtual2021]


## Target trial framework {.smaller .scrollable}

1. Suppose you are interested in conducting a pragmatic randomised controlled trial (RCT) to evaluate your health intervention vs usual care
1. However, conducting this trial is infeasible so we resort to emulating this trial with observational data
1. The target trial framework[@hernanUsingBigData2016; @hernanSpecifyingTargetTrial2016] gives us guidelines to conceptualise our study
1. An example of the application of the target trial framework[@coteIntroductionTargetTrial2024]
![Table III from Cote et al.](images/cote_2024_table3.png)


## Target trial framework pointers {.smaller .scrollable}

1. Extra pointers adapted from [@hernanUsingBigData2016; @wangTargetTrialFramework2024]

<table>
  <colgroup>
    <col style="width:20%">
    <col style="width:80%">
  </colgroup>
  <thead>
    <tr>
      <th>Protocol component</th>
      <th>Observational study that
          emulates the RCT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Eligibility criteria</td>
      <td>
        • Apply the same inclusion/exclusion criteria<br>
        • Assuming that we have variables needed for inclusion/exclusion in our observational dataset<br>
        • Eligibility should only be determined based on info available at baseline (cf. immortal time bias)<br>
        • Selection bias if we exclude patients with missing data (analogue of dropout/loss to follow-up in a trial) if dropout is associated with intervention and outcome<br>
        • Patients could be eligible at multiple time points. We have a choice of (1) single eligible time: set baseline as the first timepoint when eligibility is met within a time period or (2) all eligible times (or large subset): emulate multiple nested trials
      </td>
    </tr>
    <tr>
      <td>Treatment strategies</td>
      <td>
        • Assign patients to strategy consistent with their baseline data<br>
        • Ensure treatment arms are consistent e.g. both arms contain only new initiators of a treatment as opposed to one arm having patients who are already receiving treatment some time before baseline, which affects the outcome<br>
        • Positivity: _"each individual should have positive probability of receving each level of exposure for every combination of covariates"_
          (i.e. a patient should not be precluded from a level of exposure to the intervention at the outset)<br>
        • Consistency: _"there cannot be two versions of a treatment that would result in different outcomes"_
      </td>
    </tr>
    <tr>
      <td>Assignment procedures</td>
      <td>
        • Can only emulate trials without blind assignment because individuals/care providers in the dataset
          are aware of the received treatments<br>
        • Patients are not randomly assigned to treatment strategies. methods to achieve exchangeability at baseline e.g. propensity score matching, inverse probability weighting and g-methods to deal with time-varying confounding<br>
        • Limitation: residual confounding from unmeasured confounders
      </td>
    </tr>
    <tr>
      <td>Follow-up period</td>
      <td> </td>
    </tr>
    <tr>
      <td>Outcome</td>
      <td> </td>
    </tr>
    <tr>
      <td>Causal contrast of interests</td>
      <td>
        • Intention-to-treat effect: we would use patients who were prescribed medication for treatment initiation<br>
        • Per-protocol effect: we would use patients who adhered to the treatment. Adjustment for postbaseline confounding is needed because postbaseline prognostic factors associated with subsequent adherence to treatment may be affected by prior adherence
      </td>
    </tr>
    <tr>
      <td>Analysis plan</td>
      <td>
        • Proper definition of baseline/time zero is important (affects assessment of eligibility)<br>
        • Best way to define time zero is the time time when an eligible individual initiates a treatment strategy
      </td>
    </tr>
  </tbody>
</table>


## Propensity score matching {.smaller}

1. What is a propensity score?
1. What is the purpose of propensity score matching?
1. How is propensity score estimated?
1. How is propensity score matching done?

## How do we choose matching variables?


## How do we evaluate covariate balance after matching?

1. We make use of the absolute _standardized mean difference (SMD)_ metric
1. Rule of thumb
1. p-values are not used because




# Analysis: Intervention effect estimation



# Analysis: Missing data

## Outline

1. Why do we care about missing data?
1. What are the types of missingness mechanisms?
1. What are the implications of each missingness mechanism?
1. Multiple imputation as a method for imputing missing data

## Motivation: why do we care about missing data? {.smaller}

1. Commonly used statistical models (e.g. GLM) require complete-cases $\rightarrow$ forced to discard data points
1. Data is expensive to collect
1. In throwing out data, you will face the problem of:
    a. Efficiency: number of data points are reduced $\rightarrow$ lose statistical power
    a. Bias: you might systematically exclude a population group and end up with a biased dataset
    a. Both bias and efficiency will influence your final conclusion

## Missingness mechanism

1. A <ins>missingness mechanism</ins> refers to the underlying process which generated the missing data
1. There are 3 missingness mechanisms
    i. Missing completely at random (MCAR)
    i. Missing at random (MAR)
    i. Missing not at random (MNAR)

## Framework for understanding missingness mechanism

1. We will use the framework as laid out in Mohan & Pearl[@mohanGraphicalModelsProcessing2021]
1. Reason: Clearest way to discriminate between MCAR, MAR, MNAR
1. Prerequisite: This requires an understanding of the concept of d-separation in graphical models

## Framework notation {.smaller .scrollable}

Suppose we have data in the following format[@mohanGraphicalModelsProcessing2021]
![](images/mohan_pearl_table2.png)

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1b.png){fig-align="center"}

::: {.r-fit-text}
- $A$: Age
- $G$: Gender
- $O$: Obesity \text{(partially observed)}
- $O^*$: Obesity \text{(fully observed)}
- $R_O$: Missing indicator
:::
:::

::: {.column width="75%"}
1. Fully observed variables are shaded (e.g. $A$, $G$)
1. Partially observed variables (with missing data) are non-shaded (e.g. $O$)
1. To model the missingness process, two variables are introduced:
    i. $R_O$ - A binary {0, 1} masking variable that governs missingness
    i. $O^*$ - A proxy variable for obesity that is fully observed
    $$ O^* = f(R_O, O) = 
        \begin{cases}
            O  & \text{if } R_O = 0 \\
            m  & \text{if } R_O = 1
        \end{cases} $$
:::
:::

## #1 Missing completely at random (MCAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1b.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing obesity data is MCAR if the cause of missingness is independent of any other variable (There are no edges between $R_O$ and $G, A, O$)
1. Examples of how Obesity could be MCAR:
    i. Technical fault: eg. random connectivity downtime in the weighing machine that prevents the sending of patient’s weight to EMR
    i. Workflow: patient forgets to take his/her weight after questionnaire (and we could not call the patient to get his/her weight after)
:::
:::

### Implications of MCAR

1. Complete-case analysis $\rightarrow$ <ins>unbiased</ins> dataset, though loss of statistical power due to fewer $N$
1. Multiple imputation $\rightarrow$ <ins>unbiased</ins> dataset
    i. Multiple imputation attempts to impute missing variables using a model of the partially observed variable ($Y$) on the observed variables ($X$)
    i. Why unbiased? $R_O \perp\!\!\!\!\!\!\perp O$ implies that $P(O | A, G, R_O = 1) = P(O | A, G, R_O = 0)$
    i. The distribution of missing obesity values (which we are trying to impute) is the same as the distribution of observed obesity values, given the other observed variables like age & gender
    i. So we can estimate the missing obesity values using a logistic regression model

## #2 Missing at random (MAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1c.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing Obesity data is MAR if the cause of missingness is dependent on fully observed variable(s) eg. Age
1. Conditional independence assertion:
    i. Missingness, $R_O$, is conditionally independent of Obesity given Age i.e. $R_O \perp\!\!\!\!\!\!\perp O | A$
    i. Or you can think of it as: missingness, $R_O$, is only dependent on Age
1. Example of how Obesity could be MAR:
    i. A lot of missing weight data among teenagers who were rebellious and did not report their weight
:::
:::

### Implications of MAR

1. Complete-case analysis $\rightarrow$ <ins>biased</ins> dataset 
    i. Why biased? Data from teenage patients is excluded
1. Multiple imputation $\rightarrow$ <ins>unbiased</ins> dataset
    i. Why unbiased? Data from teenage patients is imputed and not excluded
    i. The conditional independence assertion above implies that $P(O | A, G, R_O = 1) = P(O | A, G, R_O = 0)$
    i. We can impute the missing data as the distribution of missing obesity values (which we are trying to impute) is the same as the distribution of observed obesity values, given the other observed variables like age & gender

## #3 Missing not at random (MNAR) {.smaller .scrollable}

::: {.columns}

::: {.column width="25%"}
![](images/mohan_pearl_figure1d.png){fig-align="center"}
:::

::: {.column width="75%"}
1. We say that the missing Obesity data is MNAR if the cause of missingness is dependent on the Obesity variable itself
1. Example of how Obesity could be MNAR:
    i. A lot of missing weight data from obese patients who are embarrassed and did not disclose their weight 
    i. In another setting, we often find that low/high income earners do not disclose their income
:::
:::

### Implications of MNAR

1. Complete case analysis $\rightarrow$ <ins>biased</ins> dataset
    i. Why biased? Data from obese patients is excluded
1. Multiple imputation $\rightarrow$ <ins>biased</ins> dataset
    i. Why biased? The distribution of missing obesity values (which we are trying to impute) is different from the distribution of observed obesity values, given the other observed variables like age & gender i.e. $P(O | A, G, R_O = 1) \ne P(O | A, G, R_O = 0)$
    i. Imputing missing weight data for obese patients using data from patients who are not obese $\rightarrow$ imputed weight tend to be lower than what it should actually be
1. Imputation of missing data requires causal modelling of $R$ variables. No pure statistical method of imputation exists[@mohanGraphicalModelsProcessing2021]

## Can these missingness mechanisms be tested for? {.smaller}

1. MAR and MNAR cannot be tested for (see section 1.4 of Enders[@endersAppliedMissingData2022])
    1. In each case, an independence assertion regarding the missing variable was made
    1. Since the missing variable is by definition partially unobserved, we cannot do a test for such an assertion where the input to the test is unobserved
1. MCAR can be tested for (see section 1.9 of Enders[@endersAppliedMissingData2010])
    1. There are testable propositions, the idea is to assume that the cases with missing data came from the same population and thus have the same means and covariances as the complete cases
    1. Little's test is one test for MCAR that evaluates the mean differences across subgroups of cases that share the same missing data pattern
    1. The null hypothesis for Little's test states that the data are MCAR, so a statistically significant test statistic provides evidence against the MCAR mechanism

## Caveats about Little's test {.smaller}

> From section 1.9 of Enders[@endersAppliedMissingData2010]
>
> 1. The test does not identify the specific variables that violate MCAR, so it is only useful for testing an omnibus hypothesis that is unlikely to hold in the first place
> 1. The version of the test outlined above assumes that the missing data patterns share a common covariance matrix. MAR and MNAR mechanisms can produce missing data patterns with different variances and covariances, and the test statistic in Equation 1.4 would not necessarily detect covariance based deviations from MCAR
> 1. Simulation studies suggest that Little’s test suffers from low power, particularly when the number of variables that violate MCAR is small, the relationship between the data and missingness is weak, or the data are MNAR. Consequently, the test has a propensity to produce Type II errors and can lead to a false sense of security about the missing data mechanism

## Multiple imputation by chained equations (MICE)

## Conclusion

1. We have learnt about the 3 missingness mechanisms of MCAR, MAR, MNAR
1. We have considered the implications of *complete-case analysis* and *multiple imputation* on bias & efficiency
1. Challenge remains: a thorough handling of missing data requires causal modelling in order to:
    i. Distinguish between the 3 missingness mechanisms
    i. Impute missing data in MNAR scenario where common methods like complete-case analysis or multiple imputation are invalid


# Presentation of results {#presentation}

## Markdown

1. Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world’s most popular markup languages.[^3]
1. Try out the [markdown live preview](https://markdownlivepreview.com/){target="_blank"}

[^3]: From [Markdown Guide](https://www.markdownguide.org/getting-started/){target="_blank"}


## R Markdown {.smaller}

1. R Markdown is an extension to markdown that enables user to execute R code and display the code output in addition to text
1. The [knitr](https://yihui.org/knitr/){target="_blank"} application enables us to execute R code and display output in a temporary markdown document
1. [Pandoc](https://pandoc.org/){target="_blank"} then converts the markdown document to the desired format (e.g. html, pdf, docx, pptx)
1. See summary by [Robin Linacre](https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown){target="_blank"}

```{r}
#| echo: false
#| out-width: "40%" 
#| fig-cap: Image from [R Markdown cookbook chapter 2.1](https://bookdown.org/yihui/rmarkdown-cookbook/rmarkdown-process.html){target="_blank"}
#| fig-align: left
knitr::include_graphics(here("images/rmarkdown_knitr_process.png"))
```


## [DEMO] Creation of an R markdown document {.smaller .scrollable}

1.  Create an rmarkdown document

    ![](images/create_rmarkdown.gif){width="70%"}

1.  Knit your document into html

    ![](images/knit_rmarkdown.gif){width="70%"}


## Components of Rmarkdown document {.smaller .scrollable}

1. YAML header: controls the meta data e.g. Title, date, output document format, table of contents
1. Formatted text of your descriptions, explanations etc.
1. Code chunks: where you run R code e.g. plot graphs and display the output in the same document

::: {style="text-align:center;"}
```{r}
#| echo: false
#| out-height: "120%"
#| out-width: "120%" 
#| fig-cap: Image taken from [An Introduction to R](https://intro2r.com/r-markdown-anatomy.html){target="_blank"}
#| fig-align: "center" 
knitr::include_graphics(here("images/rmd_anatomy.png"))
```
:::


## R markdown YAML header {.smaller}

```{r}
#| echo: true
#| eval: false
---
title: "My project"
author: "CRU"
date: '`r format(Sys.Date(), "%d %b %Y")`'
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
bibliography: references.bib
csl: vancouver-superscript.csl 
link-citations: true
---
```

1. This is a common setting which I use 
1. [bookdown::html_document2](https://bookdown.org/yihui/bookdown/a-single-document.html){target="_blank"} of the [Bookdown](https://pkgs.rstudio.com/bookdown/){target="_blank"} package is a good output format because it takes care of formatting e.g. auto-numbering of sections, handling of references, appendix
1. toc (line 7 & 8) refers to table of contents
1. bibliography, csl and link-citations is for adding citations stored in a bibtex file into your document. More will be covered in a later slide


## Consort diagram {.smaller .scrollable}

1.  The [`consort`](https://cran.r-project.org/web/packages/consort/index.html) package provides a convenient `consort_plot` function to plot CONSORT diagrams for reporting inclusion/exclusion/allocation (see the [vignette](https://cran.r-project.org/web/packages/consort/vignettes/consort_diagram.html))

    ```{r}
    #| out-height: "100%"
    #| out-width: "120%"
    library(consort)
    flow_diagram <- consort_plot(data = penguins %>%
                                          mutate(id = row_number(),
                                                 exclusion = case_when(island == "Dream" ~ "Penguins on Dream island",
                                                                       year == 2007 ~ "Data collected in 2007",
                                                                       TRUE ~ NA_character_) %>%
                                                             fct_relevel("Penguins on Dream island",
                                                                         "Data collected in 2007")),
                                 orders = c(id = "Study population",
                                            exclusion = "Excluded",
                                            id = "Data for analysis"),
                                            side_box = "exclusion",
                                 cex = 1.2)
    plot(flow_diagram)
    ```

1.  For presenting in html documents, the plot function does not render the plot fully so when that happens, save it as an image first

    ```{r}
    #| eval: false
    # Change the file path to where you want your image to be saved
    ggsave(here("images/consort_diagram.png"), plot = build_grid(flow_diagram))
    ```

1.  Next, load your image using `knitr::include_graphics`

    ```{r, fig.height = 2}
    #| eval: false
    # You can use the fig.height or fig.width chunk option to rescale your image
    knitr::include_graphics(here("images/consort_diagram.png"))
    ```


## Presenting tables {#pres_table .scrollable}

::: {.r-fit-text}

```{r}
#| code-line-numbers: "|8"
pacman::p_load(arsenal, knitr)

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE) %>%
knitr::kable(align = c("l", rep("r", 5))) # <1>
```

1. The [`kable`](https://bookdown.org/yihui/rmarkdown-cookbook/kable.html){target="_blank"} function converts the table object to html

:::


## Formatting HTML tables {.scrollable}

The [kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html){target="_blank"} package provides useful functions to format tables e.g. changing column width, changing colors, font size, fixed header rows etc.

::: {.r-fit-text}

```{r}
#| code-line-numbers: "|9-11"
pacman::p_load(arsenal, knitr, kableExtra)

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE) %>%
kable(align = c("l", rep("r", 5))) %>%
kableExtra::column_spec(column = 1, width_min = "4.75cm") %>%
kableExtra::column_spec(column = 2:5, width_min = "5.75cm") %>%
kableExtra::column_spec(column = 6, width_min = "2.25cm")
```

:::


## Formatting HTML table subheaders {.scrollable}

::: {.r-fit-text}

If you added labels to your dataframe using the [`labelled`](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html){target="_blank"} package, it can be presented as subheaders via the `labelTranslations` argument in `summary`

```{r}
#| code-line-numbers: "1-4||10"
labels <- list(sex = "Sex",
               island = "Island",
               bill_length_mm = "Bill length (mm)")
labelled::var_label(penguins) <- labels

tableby(species ~ sex + island + bill_length_mm,
        data = penguins,
        control = tableby.control(numeric.stats = c("Nmiss", "meansd", "medianq1q3", "range"),
                                  digits = 2)) %>%
summary(text = TRUE, labelTranslations = map(labelled::var_label(penguins), ~ str_glue("**{.x}**"))) %>% # <1>
kable(align = c("l", rep("r", 5))) %>%
kableExtra::column_spec(column = 1, width_min = "5.5cm") %>%
kableExtra::column_spec(column = 2:5, width_min = "5.75cm") %>%
kableExtra::column_spec(column = 6, width_min = "2.25cm")
```

1. The ** will render the text bold in Rmarkdown html documents

:::


## Cross-references & citations

1. See [cross-references](https://bookdown.org/yihui/bookdown/cross-references.html){target="_blank"} and [citations](https://bookdown.org/yihui/bookdown/citations.html){target="_blank"} on how to add internal cross-references and citations from a bibtex file
1. See [bibliographies and citations](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html){target="_blank"} on how to add a references and appendix section


## Reflection questions {.scrollable}

::: {.r-fit-text}

-	Can you provide a brief overview of your research and its main objectives?
-	What was your rationale for choosing this particular research topic?
-	How did you develop your research questions or hypotheses?
- What gaps in the literature is your study intended to address?
-	Can you explain your chosen methodology and why it was appropriate for your study?
-	What were the main challenges you encountered during your research, and how did you overcome them?
-	How does your research contribute to the existing body of knowledge in your field?
-	Can you discuss the key theories or concepts that informed your research?
-	Were there any conflicting views in the literature, and how did you address them?
-	How did you ensure that your literature review was comprehensive and up-to-date?
-	Can you explain your data collection process and any ethical considerations?
-	What were the main findings of your research?
-	How did you analyze your data, and why did you choose those specific analytical methods?
- How did you choose the variables in your model?
-	Were there any unexpected results, and how did you interpret them?
-	How do your findings relate to previous research in the field?
-	What are the practical implications of your research?
-	How might your findings be applied in real-world settings?
-	What are the limitations of your study, and how might they be addressed in future research?
-	If you were to continue this research, what would be your next steps?
-	Looking back, what would you do differently if you were to conduct this research again?
-	How has this research process contributed to your development as a researcher?
-	Can you identify any potential biases in your research and how you addressed them?
-	How do you see your research fitting into the broader context of your field?
-	Can you elaborate on any assumptions you made during your research?
-	How confident are you in your conclusions, and what evidence supports them?

:::



# References {.smaller}
